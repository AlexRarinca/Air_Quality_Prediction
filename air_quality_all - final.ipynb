{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcad1b10",
   "metadata": {},
   "source": [
    "# O3 Air Pollution Predictive Modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cdcde6",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38213399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# machine laerning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af61d1",
   "metadata": {},
   "source": [
    "## Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c8519",
   "metadata": {},
   "source": [
    "Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524dd7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    datetime StationId                     StationName       City  \\\n",
      "0 2017-11-24     AP001  Secretariat, Amaravati - APPCB  Amaravati   \n",
      "1 2017-11-25     AP001  Secretariat, Amaravati - APPCB  Amaravati   \n",
      "2 2017-11-26     AP001  Secretariat, Amaravati - APPCB  Amaravati   \n",
      "3 2017-11-27     AP001  Secretariat, Amaravati - APPCB  Amaravati   \n",
      "4 2017-11-28     AP001  Secretariat, Amaravati - APPCB  Amaravati   \n",
      "\n",
      "            State        lat        lon  Status  PM2.5    PM10  ...  Humidity  \\\n",
      "0  Andhra Pradesh  16.494222  80.510586  Active  71.36  115.75  ...  0.016259   \n",
      "1  Andhra Pradesh  16.494222  80.510586  Active  81.40  124.50  ...  0.013991   \n",
      "2  Andhra Pradesh  16.494222  80.510586  Active  78.32  129.06  ...  0.013252   \n",
      "3  Andhra Pradesh  16.494222  80.510586  Active  88.76  135.32  ...  0.013289   \n",
      "4  Andhra Pradesh  16.494222  80.510586  Active  64.18  104.09  ...  0.012934   \n",
      "\n",
      "   Temperature  Windspeed          PSurf  Rainf  month  day  dayofweek  year  \\\n",
      "0    27.045052   1.750163  100428.265833    0.0     11   24          4  2017   \n",
      "1    25.193800   2.270950  100576.689688    0.0     11   25          5  2017   \n",
      "2    24.967885   2.104017  100610.897479    0.0     11   26          6  2017   \n",
      "3    24.788809   1.791358  100468.613063    0.0     11   27          0  2017   \n",
      "4    24.984699   1.812935  100518.300792    0.0     11   28          1  2017   \n",
      "\n",
      "   stationid_int  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv('merged_data_v2.csv', low_memory=False)\n",
    "\n",
    "# Parse datetime\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], dayfirst=False)\n",
    "\n",
    "# Extract useful time features\n",
    "#df['hour'] = df['datetime'].dt.hour\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['year'] = df['datetime'].dt.year\n",
    "# convert StationId to integer labels\n",
    "df['stationid_int'], uniques = pd.factorize(df['StationId'])\n",
    "\n",
    "df['Temperature'] = df['Temperature'] - 273\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "\n",
    "# for one hot encoding stationid_int- was not successful\n",
    "#one_hot_df = pd.get_dummies(df, columns = ['stationid_int'])\n",
    "#one_hot_df['stationid_int'] = df['stationid_int'] \n",
    "#print(one_hot_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11436b4e",
   "metadata": {},
   "source": [
    "Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c05c3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric variables\n",
    "def summary_stats(df):\n",
    "\n",
    "    # print nimber of stations\n",
    "    print(f\"Number of unique stations: {df['StationId'].nunique()}\")\n",
    "    \n",
    "    print(\"Descriptive Statistics:\")\n",
    "    # for hour\n",
    "    #df_desc = df.drop(['hour', 'month', 'dayofweek', 'datetime', 'lat', 'lon', 'day'], axis=1)\n",
    "\n",
    "    df_desc = df.drop(['month', 'dayofweek', 'datetime', 'lat', 'lon', 'day'], axis=1)\n",
    "    print(df_desc.describe())\n",
    "\n",
    "    print(\"Missing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "#summary_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f9a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def o3_stats(df):\n",
    "    # Print O3 statistics\n",
    "    print(f\"\\nO3 Statistics:\")\n",
    "    print(f\"Mean O3: {df['O3'].mean():.2f}\")\n",
    "    print(f\"Median O3: {df['O3'].median():.2f}\")\n",
    "    print(f\"Min O3: {df['O3'].min():.2f}\")\n",
    "    print(f\"Max O3: {df['O3'].max():.2f}\")\n",
    "\n",
    "#o3_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6d4f2",
   "metadata": {},
   "source": [
    "Map visualsation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf437b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_vis(df):\n",
    "\n",
    "    # Load Natural Earth countries (1:110m resolution)\n",
    "    url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "    world = gpd.read_file(url)\n",
    "\n",
    "    # Filter for India\n",
    "    india = world[world[\"ADMIN\"] == \"India\"]\n",
    "\n",
    "\n",
    "    # grouping by city as hard to see individual points otherwise\n",
    "\n",
    "    # groups by city and takes mean of O3 values\n",
    "    df_mean = df.groupby(['City']).mean('O3').reset_index()\n",
    "    # Remove rows with NaN values in or 'O3' columns\n",
    "    df_clean = df_mean.dropna(subset=[\"O3\"])\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_clean,  # remove NaNs\n",
    "        geometry=gpd.points_from_xy(df_clean[\"lon\"], df_clean[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    india.plot(ax=ax, color=\"white\", edgecolor=\"black\")  # India boundary\n",
    "\n",
    "    # Scatter plot of O3 levels\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column=\"O3\",        # pollutant column\n",
    "        cmap=\"coolwarm\",        # color scheme\n",
    "        markersize=80,      # adjust dot size\n",
    "        alpha=0.8,\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "        \"label\": \"O₃ Concentration (µg/m³)\",   # ← your legend label\n",
    "        #\"shrink\": 0.6                         # optional: smaller colorbar\n",
    "    }\n",
    "    )\n",
    "\n",
    "    plt.title(\"Map of Ozone (O₃) Levels Across India\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#map_vis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531452f",
   "metadata": {},
   "source": [
    "Visualising time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd1b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ozone_time_series(df):\n",
    "    \n",
    "    # group by day (drop the time component)\n",
    "    df_mean = (\n",
    "        df.groupby(df['datetime'].dt.date)['O3']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # rename for clarity\n",
    "    df_mean.columns = ['date', 'O3']\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.lineplot(data=df_mean, x='date', y='O3', alpha = 0.7)\n",
    "    plt.title('Daily Mean Ozone (O₃) Levels Across India')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('O₃ Concentration (µg/m³)')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(alpha = 0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def all_pollutatants_time_series(df):\n",
    "\n",
    "    pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    monthly_mean = (\n",
    "    df.groupby(pd.Grouper(key='datetime', freq='D'))[pollutants]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "    )\n",
    "\n",
    "    # plot on different graphs for clarity\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for i, pollutant in enumerate(pollutants, 1):\n",
    "        plt.subplot(3, 3, i)\n",
    "        sns.lineplot(data=monthly_mean, x='datetime', y=pollutant)\n",
    "        plt.title(f'Daily Mean {pollutant} Levels Across India')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration (µg/m³)')\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#ozone_time_series(df)\n",
    "#all_pollutatants_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403c43e",
   "metadata": {},
   "source": [
    "Random station ozone time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0044271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_station_ozone_time_series(df):\n",
    "\n",
    "    # select a random city\n",
    "    random_station = df['StationId'].sample(n=1).values[0]\n",
    "\n",
    "    df_station = df[df['StationId'] == random_station]\n",
    "\n",
    "    # group by day (drop the time component)\n",
    "    df_mean = (\n",
    "        df_station.groupby(df_station['datetime'].dt.date)['O3']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # rename for clarity\n",
    "    df_mean.columns = ['date', 'O3']\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.lineplot(data=df_mean, x='date', y='O3')\n",
    "\n",
    "    plt.title(f'Daily Mean Ozone (O₃) Levels in {df_station[\"City\"].values[0]} (Station {random_station})')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('O₃ Concentration (µg/m³)')\n",
    "    plt.grid(alpha=0.7)\n",
    "\n",
    "    # Format x-axis to show only years\n",
    "    ax = plt.gca()  # get current axis\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())  # one tick per year\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# can return a city with no ozone data so may need to rerun to find one that does\n",
    "#random_station_ozone_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d3f32",
   "metadata": {},
   "source": [
    "Seasonal Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe03a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_fig(df):\n",
    "    # Extract year from datetime\n",
    "    # copy to not impact original df\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Year'] = df_copy['datetime'].dt.year\n",
    "    # # Year-over-Year O3 Trend\n",
    "    yearly_O3 = df_copy.groupby('Year')['O3'].mean().reset_index()\n",
    "\n",
    "    #print(\"\\nYearly O3 Summary:\")\n",
    "    #print(yearly_O3)\n",
    "\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.plot(yearly_O3['Year'], yearly_O3['O3'], marker='o', linewidth=2, \n",
    "             markersize=8, color='darkred')\n",
    "    plt.title('Year-over-Year O3 Trend', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Average O3', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(yearly_O3['Year'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def monthly_fig(df):\n",
    "    # Month-over-Month O3 Trend\n",
    "    monthly_O3 = df.groupby('month')['O3'].mean().reset_index()\n",
    "\n",
    "    #print(\"\\nMonthly O3 Summary:\")\n",
    "    #print(monthly_O3)\n",
    "\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.plot(monthly_O3['month'], monthly_O3['O3'], marker='o', linewidth=2, \n",
    "            markersize=8, color='darkred')\n",
    "    plt.title('Month-over-Month O3 Trend', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Month', fontsize=12)\n",
    "    plt.ylabel('Average O3', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(monthly_O3['month'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def day_of_week_fig(df):\n",
    "    # Day of the week O3 Trend\n",
    "    day_of_week_O3 = df.groupby('dayofweek')['O3'].mean().reset_index()\n",
    "\n",
    "    #print(\"\\nDaily O3 Summary:\")\n",
    "    #print(day_of_week_O3)\n",
    "\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.plot(day_of_week_O3['dayofweek'], day_of_week_O3['O3'], marker='o', linewidth=2, \n",
    "            markersize=8, color='lightgreen')\n",
    "    plt.title('Day-Of-The-Week O3 Trend', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('DayOfWeek', fontsize=12)\n",
    "    plt.ylabel('Average O3', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(day_of_week_O3['dayofweek'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def seasonal_trends(df):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['weekday_name'] = df_copy['datetime'].dt.day_name()\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(6,5))\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        else:\n",
    "            return 'Autumn'\n",
    "\n",
    "    df_copy['season'] = df_copy['month'].apply(get_season)\n",
    "\n",
    "    seasonal_summary = df_copy.groupby('season')['O3'].mean().reset_index()\n",
    "    sns.barplot(x='season', y='O3', data=seasonal_summary, order=['Spring', 'Summer', 'Autumn', 'Winter'])\n",
    "    plt.title('Average Seasonal O₃ Levels')\n",
    "    plt.show()\n",
    "\n",
    "#seasonal_trends(df)\n",
    "#yearly_fig(df)\n",
    "#monthly_fig(df)\n",
    "#day_of_week_fig(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6d93f",
   "metadata": {},
   "source": [
    "Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8470bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms(df):\n",
    "    numeric_features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    # Create subplots for all histograms\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    axes = axes.flatten()  # Make indexing easier\n",
    "\n",
    "    for i, col in enumerate(numeric_features):\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].axvline(df[col].median(), color='r', linestyle='--', label='Median'),\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#histograms(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14381b",
   "metadata": {},
   "source": [
    "Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ea2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "\n",
    "    # Correlation matrix for pollutants.\n",
    "    df_corr = df.drop(['AQI_Bucket', 'City', 'datetime', 'StationId', 'StationName', 'Status', 'State', 'lat', 'lon', 'month', 'dayofweek', 'day', 'year', 'AQI', 'stationid_int'], axis=1)\n",
    "    corr_matrix = df_corr.corr()\n",
    "\n",
    "    \"\"\" # Show strong correlations with 03\n",
    "    if 'O3' in corr_matrix.columns:\n",
    "        O3_corr = corr_matrix['O3'].sort_values(ascending=False)\n",
    "        print(\"\\nCorrelation with O3:\")\n",
    "        print(O3_corr[1:11])\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "#correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9f310",
   "metadata": {},
   "source": [
    "Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa7fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_matrix(df):\n",
    "\n",
    "    # Correlation matrix for pollutants.\n",
    "    df_cov = df.drop(['AQI_Bucket', 'City', 'datetime', 'StationId', 'StationName', 'Status', 'State'], axis=1)\n",
    "    cov_matrix = df_cov.cov()\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(cov_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "#cov_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad808b28",
   "metadata": {},
   "source": [
    "Distribution of AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0633b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aqi_plots(df):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # 03 distribution\n",
    "    axes[0].hist(df['AQI'].dropna(), bins=50, color='blue', edgecolor='black')\n",
    "    axes[0].set_title('Distribution of AQI Values', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('AQI', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].axvline(df['AQI'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"AQI\"].mean():.2f}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # # AQI Bucket distribution\n",
    "    aqi_bucket_counts = df['AQI_Bucket'].value_counts()\n",
    "    axes[1].bar(aqi_bucket_counts.index, aqi_bucket_counts.values, color='red', edgecolor='black')\n",
    "    axes[1].set_title('Distribution of AQI Buckets', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('AQI Bucket', fontsize=12)\n",
    "    axes[1].set_ylabel('Count', fontsize=12)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#aqi_plots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226960f",
   "metadata": {},
   "source": [
    "Top 50 Worst days for O3 pollution all in same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2867d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_pollutant_days(df, target_pollutant = 'O3'):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Year'] = df_copy['datetime'].dt.year\n",
    "    # Get top 50 days with Highest O3\n",
    "    top_O3_days = df_copy.nlargest(50, target_pollutant)[['datetime', 'StationName',target_pollutant]]\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Top 50 days with Highest O3\")\n",
    "    print(\"=\" * 50)\n",
    "    print(top_O3_days.to_string(index=False))\n",
    "\n",
    "#top_pollutant_days(df, 'NO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a0eeb",
   "metadata": {},
   "source": [
    "Average Pollutant per stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62913463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pollutant_by_station(df):\n",
    "\n",
    "    # pollutant list\n",
    "    pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene','Toluene','Xylene']\n",
    "\n",
    "    stations_mean = (\n",
    "        df.groupby('StationId')[pollutants]\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Stations by average pollutant levels\n",
    "\n",
    "    # Create subplots: 3 rows × 3 columns\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()  # make axes 1D for easy looping\n",
    "\n",
    "    # Plot each pollutant\n",
    "    for i, pollutant in enumerate(pollutants):\n",
    "        # Sort stations for that pollutant\n",
    "        sorted_data = stations_mean[pollutant].sort_values(ascending=False)\n",
    "\n",
    "        sorted_data.plot(\n",
    "            kind='barh',\n",
    "            ax=axes[i],\n",
    "            color='orange',\n",
    "            edgecolor='black'\n",
    "        )\n",
    "        axes[i].set_title(f'Stations by Average {pollutant}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel(f'Average {pollutant} (µg/m³)', fontsize=10)\n",
    "        axes[i].set_ylabel('Station', fontsize=10)\n",
    "        axes[i].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#avg_pollutant_by_station(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50214485",
   "metadata": {},
   "source": [
    "Finding outliers in target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da319b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_in_avg_in_pollutant(df, target_pollutant = 'O3'):\n",
    "  pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "  stations_mean = (\n",
    "      df.groupby('StationName')[pollutants]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "  )\n",
    "  top_stations = (\n",
    "    stations_mean[['StationName', target_pollutant]]\n",
    "    .sort_values(by=target_pollutant, ascending=False)\n",
    "    .head(10)\n",
    "  )\n",
    "  print('Top stations', f'\\n{top_stations}')\n",
    "\n",
    "  bottom_stations = (\n",
    "    stations_mean[['StationName', target_pollutant]]\n",
    "    .sort_values(by=target_pollutant, ascending=True)\n",
    "    .head(10)\n",
    "  )\n",
    "  print('Bottom Stations',  f'\\n{bottom_stations}')\n",
    "\n",
    "#find_outliers_in_avg_in_pollutant(df, target_pollutant= 'NOx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ce251",
   "metadata": {},
   "source": [
    "Nan value analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea1cdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_by_pollutant(df):\n",
    "\n",
    "    India = df.copy()\n",
    "\n",
    "    # Keep only pollutant-related columns\n",
    "    polls = ['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI']\n",
    "\n",
    "\n",
    "    miss_pct = {}\n",
    "    for c in polls:\n",
    "        total_valid = India[c].notna().sum() + India[c].isna().sum()  \n",
    "        miss_pct[c] = (India[c].isna().sum() / total_valid) * 100\n",
    "\n",
    "    miss_pct = pd.Series(miss_pct).sort_values(ascending=False)\n",
    "    print(\"Missing Percentage by Pollutant (%):\", miss_pct)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.barplot(x=miss_pct.index, y=miss_pct.values, alpha = 0.7, width = 0.6)\n",
    "    plt.title(\"Missing Percentage by Pollutant (%)\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Missing Percentage (%)\")\n",
    "    plt.xlabel(\"Pollutant\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def nan_by_city(df):\n",
    "\n",
    "    India = df.copy()\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Calculate missing rate by city\n",
    "    # Keep only pollutant-related columns\n",
    "    polls = ['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI']\n",
    "    \n",
    "    city_miss = India.groupby('City')[polls].apply(lambda x: x.isnull().mean() * 100)\n",
    "    city_miss_mean = city_miss.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "    print(\"Top 10 Cities with Highest Missing Rates (%):\")\n",
    "    print(city_miss_mean.head(10))\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    city_miss_mean.plot(kind='bar', alpha = 0.7)\n",
    "    plt.title(\"Average Missing Rate by City (%)\")\n",
    "    plt.ylabel(\"Missing Percentage (%)\")\n",
    "    plt.xlabel(\"City\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(\n",
    "        city_miss,\n",
    "        cmap='YlGnBu',       \n",
    "        linewidths=0.3,       \n",
    "        linecolor='white',\n",
    "        cbar_kws={'label': 'Missing Rate (%)'},\n",
    "        annot=True,             \n",
    "        fmt=\".1f\",            \n",
    "        annot_kws={'size':8, 'color':'black'} \n",
    "    )\n",
    "\n",
    "    plt.title(\"City vs Pollutant Missing Rate Heatmap\", fontsize=16, fontweight='bold', pad=12)\n",
    "    plt.xlabel(\"Pollutant\", fontsize=12)\n",
    "    plt.ylabel(\"City\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def nan_by_station(df):\n",
    "\n",
    "    India = df.copy()\n",
    "    #  Calculate missing rate by monitoring station\n",
    "\n",
    "    polls = ['NO2', 'NO'] #['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI']\n",
    "\n",
    "    station_miss = India.groupby('stationid_int')[polls].apply(lambda x: x.isnull().mean() * 100)\n",
    "    station_miss['Average'] = station_miss.mean(axis=1)\n",
    "    station_miss = station_miss.sort_values('Average', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Monitoring Stations with Highest Missing Rates:\")\n",
    "    print(station_miss.head(10))\n",
    "\n",
    "def nan_percent_by_station(df):\n",
    "\n",
    "    # pollutant list\n",
    "    pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    # Calculate % of NaN values for each pollutant per station\n",
    "    stations_nan_pct = (\n",
    "        df.groupby('StationId')[pollutants]\n",
    "          .apply(lambda x: x.isna().mean() * 100)\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create subplots: 3 rows × 3 columns\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()  # make axes 1D for easy looping\n",
    "\n",
    "    # Plot each pollutant\n",
    "    for i, pollutant in enumerate(pollutants):\n",
    "        # Sort stations by % NaN for that pollutant\n",
    "        sorted_data = stations_nan_pct.sort_values(by=pollutant, ascending=False)\n",
    "\n",
    "        axes[i].barh(sorted_data['StationId'], sorted_data[pollutant],\n",
    "                     color='skyblue', edgecolor='black')\n",
    "\n",
    "        axes[i].set_title(f'% NaN in {pollutant}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('% Missing Values', fontsize=10)\n",
    "        axes[i].set_ylabel('Station', fontsize=10)\n",
    "        axes[i].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#nan_by_pollutant(df)\n",
    "#nan_by_city(df)\n",
    "#nan_by_station(df)\n",
    "#nan_percent_by_station(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be5101",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08750090",
   "metadata": {},
   "source": [
    "data set prep and splitting for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed3c4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(df, X_feature_columns, target_pollutant = 'O3'):   \n",
    "    \"\"\"\n",
    "    Function to normalise selected features in the dataframe.\n",
    "    \"\"\"\n",
    "    # only normalise pollutant values\n",
    "    col_to_norm = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "    col_to_norm.remove(target_pollutant)\n",
    "\n",
    "    X = df[X_feature_columns]  # Can contain NaN values\n",
    "    y = df[[target_pollutant]]  # Should not contain NaN\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Normalised dataset \n",
    "    X_norm = X.copy()\n",
    "    # Scale only selected columns\n",
    "    scaler_X = preprocessing.MinMaxScaler()\n",
    "    X_norm[col_to_norm] = scaler_X.fit_transform(X[col_to_norm])\n",
    "    scaler_y = preprocessing.MinMaxScaler()\n",
    "    y_norm = scaler_y.fit_transform(y)\n",
    "\n",
    "    y_norm_df = pd.DataFrame(y_norm, columns=[target_pollutant], index=y.index)\n",
    "\n",
    "    df_norm = pd.concat([X_norm, y_norm_df], axis=1)\n",
    "\n",
    "    return df_norm, scaler_y\n",
    "\n",
    "def data_splitting(df, X_feature_columns, target_pollutant = 'O3', how = 'by_station'):\n",
    "\n",
    "    df_split = df.copy()\n",
    "\n",
    "    if how == 'by_station':\n",
    "        # get unique station names\n",
    "        stations = df['stationid_int'].unique()\n",
    "        # shuffle stations\n",
    "        #np.random.seed(42)  # for reproducibility\n",
    "        np.random.shuffle(stations)\n",
    "        # split stations into train and test (80-20)\n",
    "        split_index = int(0.8 * len(stations))\n",
    "\n",
    "        \n",
    "        train_stations = stations[:split_index]\n",
    "        test_stations = stations[split_index:]\n",
    "        # create train and test sets based on stations\n",
    "        train_set = df_split[df_split['stationid_int'].isin(train_stations)]\n",
    "        test_set = df_split[df_split['stationid_int'].isin(test_stations)]\n",
    "\n",
    "    elif how == 'by_time':\n",
    "        # Sort by datetime (important for time-based splits)\n",
    "        df_split = df_split.sort_values('datetime')\n",
    "\n",
    "        # Calculate the index at the 80% mark\n",
    "        split_index = int(len(df_split) * 0.8)\n",
    "\n",
    "        # Split\n",
    "        train_set = df_split.iloc[:split_index]\n",
    "        test_set  = df_split.iloc[split_index:]\n",
    "\n",
    "\n",
    "    elif how == 'random':\n",
    "        # Shuffle the dataset randomly\n",
    "        df_split = df_split.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        # Calculate the index at the 80% mark\n",
    "        split_index = int(len(df_split) * 0.8)\n",
    "\n",
    "        # Split into train and test sets\n",
    "        train_set = df_split.iloc[:split_index]\n",
    "        test_set  = df_split.iloc[split_index:]\n",
    "\n",
    "        # Sort each subset by datetime for chronological order\n",
    "        train_set = train_set.sort_values('datetime').reset_index(drop=True)\n",
    "        test_set  = test_set.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # split into X and y\n",
    "    X_train, X_test = train_set[X_feature_columns], test_set[X_feature_columns]\n",
    "    y_train, y_test = train_set[target_pollutant], test_set[target_pollutant]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def removing_outlier_in_target(df, target_pollutant):\n",
    "\n",
    "    # Removing extreme outliers seen in avg_pollutant_per_station\n",
    "\n",
    "    if target_pollutant == 'O3':\n",
    "        \n",
    "        df_no_outlier = df[df['StationName'] != 'Punjabi Bagh, Delhi - DPCC']\n",
    "\n",
    "    elif target_pollutant == 'CO': \n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Maninagar, Ahmedabad - GPCB']\n",
    "\n",
    "    elif target_pollutant == 'NO':\n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Samanpura, Patna - BSPCB']\n",
    "\n",
    "    elif target_pollutant == 'NOx':\n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Anand Vihar, Delhi - DPCC']\n",
    "        df_no_outlier = df_no_outlier[df_no_outlier['StationName'] != 'Samanpura, Patna - BSPCB']\n",
    "\n",
    "    elif target_pollutant == 'SO2':\n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Maninagar, Ahmedabad - GPCB']\n",
    "\n",
    "    else:\n",
    "        df_no_outlier = df\n",
    "\n",
    "    return df_no_outlier\n",
    "\n",
    "def preprocess(df, feature_columns, target_pollutant = 'O3', scaler = False, splitting_method = 'by_station'):\n",
    "\n",
    "    X_features = [col for col in feature_columns if col != target_pollutant]\n",
    "    \n",
    "    # Remove rows where target variable (O3) is missing\n",
    "    df_no_Na = df.dropna(subset=[target_pollutant])\n",
    "\n",
    "    df_clean = removing_outlier_in_target(df_no_Na, target_pollutant)\n",
    "\n",
    "    if scaler == True:\n",
    "        # normaise fatures\n",
    "        df_norm, scaler_y = norm(df_clean, X_features, target_pollutant)\n",
    "    else:\n",
    "        df_norm = df_clean\n",
    "        scaler_y = None\n",
    "\n",
    "    # split data into train and test sets based on stations\n",
    "    X_train, X_test, y_train, y_test = data_splitting(df_norm, X_features, target_pollutant, how = splitting_method)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler_y\n",
    "\n",
    "def preprocess_subset(df, feature_columns, target_pollutant = 'O3', subset_size = 10, scaler = False, splitting_method = 'by_station'):\n",
    "\n",
    "    X_features = [col for col in feature_columns if col != target_pollutant]\n",
    "\n",
    "    # Remove rows where target variable (O3) is missing\n",
    "    df_no_Na = df.dropna(subset=[target_pollutant])\n",
    "\n",
    "    df_clean = removing_outlier_in_target(df_no_Na, target_pollutant)\n",
    "\n",
    "    if scaler == True:\n",
    "        # normaise fatures\n",
    "        df_norm, scaler_y = norm(df_clean, X_features, target_pollutant)\n",
    "\n",
    "    else:\n",
    "        df_norm = df_clean\n",
    "        scaler_y = None\n",
    "\n",
    "    # Get n random stations\n",
    "    subset_stations = np.random.choice(df_norm['stationid_int'].unique(), size= subset_size, replace=False)\n",
    "\n",
    "    # Filter\n",
    "    subset_df = df_norm[df_norm['stationid_int'].isin(subset_stations)]\n",
    "\n",
    "    # split data into train and test sets based on stations\n",
    "    X_train, X_test, y_train, y_test = data_splitting(subset_df, X_features, target_pollutant, how = splitting_method)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c201a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------\n",
    "# features not using the station id\n",
    "columns = df.columns.tolist()\n",
    "cols_to_drop = ['lat', 'lon', 'State', 'City', 'StationId', 'Status', 'AQI', 'AQI_Bucket','dayofweek', 'StationName']\n",
    "all_feature_columns = [c for c in columns if c not in cols_to_drop]\n",
    "\n",
    "# target pollutant for testing\n",
    "#target_pol = 'NOx'\n",
    "\n",
    "# Change target polluatant if wanting to predict for different pollutant.\n",
    "#X_train, X_test, y_train, y_test, scaler_y = preprocess(df, all_feature_columns, target_pollutant= target_pol, splitting_method= 'by_time')\n",
    "#X_train_sub, X_test_sub, y_train_sub, y_test_sub, scaler_y = preprocess_subset(df, all_feature_columns, target_pollutant= target_pol)\n",
    "\n",
    "#X_feature_columns = [f for f in all_feature_columns if f != target_pol]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83979d",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46cc614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rfecv_rf(X_train, y_train, protected_features= [], step=1, cv=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom RFECV that protects some features from elimination\n",
    "    and stores feature importances at each step.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : sklearn estimator\n",
    "    X_train : pd.DataFrame\n",
    "    y_train : pd.Series or array\n",
    "    protected_features : list of column names to never eliminate\n",
    "    step : int, number of features to remove per iteration\n",
    "    cv : int, cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_features : list of selected features\n",
    "    \"\"\"\n",
    "    if protected_features is None:\n",
    "        protected_features = []\n",
    "\n",
    "    X_work = X_train.drop(columns=['stationid_int', 'datetime']).copy()\n",
    "    features_remaining = list(X_work.columns)\n",
    "    results = []\n",
    "    vali_scores = []\n",
    "    num_pollutants_list = []\n",
    "\n",
    "    step_counter = 0\n",
    "\n",
    "    while len([f for f in features_remaining if f not in protected_features]) >= step:\n",
    "\n",
    "        num_pollutants_list.append(len([f for f in features_remaining if f not in protected_features]))\n",
    "        \n",
    "        print(f'Testing with {len([f for f in features_remaining if f not in protected_features])} features')\n",
    "\n",
    "        estimator = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Compute CV score on current feature set\n",
    "        y_pred = cross_val_predict(estimator, X_work[features_remaining], y_train, cv=cv)\n",
    "        mse = mean_squared_error(y_train, y_pred)\n",
    "        vali_scores.append(mse)\n",
    "        \n",
    "        # Fit estimator\n",
    "        estimator.fit(X_work[features_remaining], y_train)\n",
    "\n",
    "        # get importances\n",
    "        importances = estimator.feature_importances_\n",
    "\n",
    "        # Store results\n",
    "        imp_df = pd.DataFrame({\n",
    "            'feature': features_remaining,\n",
    "            'importance': importances,\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        results.append({\n",
    "            'remaining_features': features_remaining.copy(),\n",
    "            'importances': imp_df\n",
    "        })\n",
    "\n",
    "        # Identify candidates for elimination\n",
    "        candidates = [f for f in features_remaining if f not in protected_features]\n",
    "        n_remove = min(step, len(candidates))\n",
    "        lowest_features = imp_df[imp_df['feature'].isin(candidates)].tail(n_remove)['feature'].tolist()\n",
    "\n",
    "        # Remove lowest importance features\n",
    "        for f in lowest_features:\n",
    "            features_remaining.remove(f)\n",
    "\n",
    "        # Optional: stop when number of features = number of protected features\n",
    "        if len([f for f in features_remaining if f not in protected_features]) <= 0:\n",
    "            break\n",
    "\n",
    "    best_performance = min(vali_scores)\n",
    "    best_idx = vali_scores.index(min(vali_scores))\n",
    "    best_num_features = num_pollutants_list[best_idx]\n",
    "\n",
    "    threshold = best_performance * 1.10  # 10% worse than best\n",
    "    close_indices = [i for i, v in enumerate(vali_scores) if v <= threshold]\n",
    "\n",
    "    if close_indices:\n",
    "        # pick the LAST step within threshold → fewest features\n",
    "        min_idx_within_10pct = max(close_indices)\n",
    "        min_features_within_10pct = num_pollutants_list[min_idx_within_10pct]\n",
    "    else:\n",
    "        # fallback to the true best if none qualify\n",
    "        min_idx_within_10pct = best_idx\n",
    "        min_features_within_10pct = best_num_features\n",
    "\n",
    "    print(f\"Optimal number of features: {best_num_features} (MSE={best_performance:.4f})\")\n",
    "    print(f\"Smallest feature count within 10% of best: {min_features_within_10pct} (Step {min_idx_within_10pct})\")\n",
    "\n",
    "\n",
    "    print(f\"Optimal number of features: {best_num_features} (MSE={best_performance:.4f})\")\n",
    "    print(f\"Smallest feature count within 10% of best: {min_features_within_10pct} (Step {min_idx_within_10pct})\")\n",
    "\n",
    "    history_df = pd.concat([step['importances'].assign(step=i) for i, step in enumerate(results)], ignore_index=True)\n",
    "\n",
    "    # Extract features from history_df at the best step\n",
    "    optimal_step_df = history_df[history_df[\"step\"] == best_idx]\n",
    "    optimal_features = optimal_step_df[\"feature\"].tolist()\n",
    "\n",
    "    print(f\"Optimal features: {optimal_features}\")\n",
    "\n",
    "    plt.plot(num_pollutants_list, vali_scores, marker='o', label='Validation MSE')\n",
    "    plt.axhline(threshold, color='r', linestyle='--', label='10% threshold')\n",
    "    plt.axvline(best_num_features, color='g', linestyle='--', label='Optimal')\n",
    "    plt.axvline(min_features_within_10pct, color='orange', linestyle='--', label='Within 10%')\n",
    "    plt.xlabel(\"Number of Features\")\n",
    "    plt.ylabel(\"Validation MSE\")\n",
    "    plt.title(f\"Random Forest Elimination Curve for {y_train.name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'RF_Elimination_Curve_for_{y_train.name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_features, results[min_idx_within_10pct]['remaining_features']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39aef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_pol = 'NOx'\n",
    "#X_train, X_test, y_train, y_test, scaler_y = preprocess_subset(df, all_feature_columns, target_pollutant= target_pol, scaler = True)\n",
    "#optimal_features_rf = custom_rfecv_rf(X_train, y_train,step=1,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4e26598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_grid_search(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    param_grid = {\n",
    "    'n_estimators' : [100], \n",
    "    'max_features': [0.6,0.8, 1],\n",
    "    'max_samples': [0.6, 0.8, 1],\n",
    "    'max_depth': [None],\n",
    "    'ccp_alpha': [0],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(n_estimators=100, random_state=1),\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True,\n",
    "        verbose =2,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train.drop(columns=['stationid_int', 'datetime']), y_train)\n",
    "\n",
    "    print(\"Best Parameters:\", grid.best_params_)\n",
    "    print(\"Best Score:\", -grid.best_score_)\n",
    "\n",
    "    return grid.best_params_, grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbee3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params_rf, best_model_rf = random_forest_grid_search(X_train_sub, X_test_sub, y_train_sub, y_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4593395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test, regr= None, feature_columns = None, print_res = True, plot_graphs = True, show_figs = False, target_pollutant = 'O3'):\n",
    "\n",
    "\n",
    "    if feature_columns is not None:\n",
    "        X_train = X_train[feature_columns]\n",
    "        X_test = X_test[feature_columns]\n",
    "\n",
    "    # Then create and fit model ONLY on training data\n",
    "    if regr == None:\n",
    "\n",
    "        regr = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state= 42,\n",
    "            n_jobs= -1\n",
    "        )\n",
    "\n",
    "        regr.fit(X_train.drop(columns=['stationid_int', 'datetime']), y_train.drop(columns=['stationid_int']))\n",
    "\n",
    "    \n",
    "    else: \n",
    "\n",
    "        regr = RandomForestRegressor(**regr.get_params())\n",
    "         # Fit directly with NaN values\n",
    "        regr.fit(X_train.drop(columns=['stationid_int', 'datetime']), y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = regr.predict(X_train.drop(columns=['stationid_int', 'datetime']))\n",
    "    y_test_pred = regr.predict(X_test.drop(columns=['stationid_int', 'datetime']))\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate(y_true, y_pred, print_result, dataset_type=\"Test\"):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        nrmse = rmse / y_true.mean()  \n",
    "        \n",
    "        # --- Correlations ---\n",
    "        pearson_r, _ = pearsonr(y_true, y_pred)\n",
    "        spearman_r, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "        if print_result == True:\n",
    "            print(f\"{dataset_type} MAE: {mae:.2f}\")\n",
    "            print(f\"{dataset_type} MSE: {mse:.2f}\")\n",
    "            print(f\"{dataset_type} RMSE: {rmse:.2f}\")\n",
    "            print(f\"{dataset_type} R²: {r2:.2f}\")\n",
    "            print(f\"{dataset_type} NRMSE: {nrmse:.2f}\")\n",
    "            print(f\"{dataset_type} Person Corr: {pearson_r:.2f}\")\n",
    "            print(f\"{dataset_type} Spearman Corr: {spearman_r:.2f}\")\n",
    "            \n",
    "        return r2, rmse, nrmse, pearson_r, spearman_r\n",
    "\n",
    "    r2_train, rmse_train, nrmse_train, pearson_train, spearman_train = evaluate(y_train, y_train_pred, print_res, \"Train\")\n",
    "    r2_test, rmse_test, nrmse_test, pearson_test, spearman_test = evaluate(y_test, y_test_pred, print_res, \"Test\")\n",
    "\n",
    "    def plotting():\n",
    "\n",
    "        # Feature Importance\n",
    "        importances = regr.feature_importances_\n",
    "        feature_names = X_train.drop(columns=['stationid_int', 'datetime']).columns\n",
    "        feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "        # plot feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "        plt.title(f'Feature Importances for {target_pollutant} from Random Forest')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Feature_importances_rf_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "        \n",
    "        random_station = np.random.choice(X_test['stationid_int'].unique())\n",
    "\n",
    "\n",
    "        mask = X_test['stationid_int'] == random_station\n",
    "        X_test_station = X_test[mask]\n",
    "        y_test_station = y_test[mask.values]\n",
    "        y_pred_station = y_test_pred[mask.values]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_test_station[-100:], label=f'Station = {random_station} Actual {target_pollutant}',  alpha=0.7)\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_pred_station[-100:], label=f'Station = {random_station} Predicted {target_pollutant}', alpha=0.7)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(f\"{target_pollutant} Levels\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.title(f'{target_pollutant} Time Series: Actual vs Predicted with RF')\n",
    "        plt.savefig(f'rf_timeseries_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "          # Combine everything into one DataFrame for convenience\n",
    "        df_results = pd.DataFrame({\n",
    "            'stationid_int': X_test['stationid_int'].values,\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_test_pred\n",
    "        })\n",
    "\n",
    "        # Compute R² for each station\n",
    "        station_r2 = (\n",
    "            df_results.groupby('stationid_int', group_keys=False)\n",
    "            .apply(lambda g: r2_score(g['y_true'], g['y_pred']), include_groups=False)\n",
    "        )\n",
    "\n",
    "        # Print top/bottom performing stations\n",
    "        print(\"Top 5 stations by R²:\")\n",
    "        print(station_r2.sort_values(ascending=False).head())\n",
    "        print(\"\\nBottom 5 stations by R²:\")\n",
    "        print(station_r2.sort_values().head())\n",
    "\n",
    "        plt.figure(figsize=(9, 5))\n",
    "        n, bins, patches = plt.hist(\n",
    "            station_r2, bins=8, color='#4C72B0', edgecolor='white', alpha=0.8, rwidth=0.9\n",
    "        )\n",
    "\n",
    "        plt.axvline(r2_test, color='red', linestyle='--', linewidth=2, label=f'Overall = {r2_test:.2f}')\n",
    "\n",
    "        # Titles and labels\n",
    "        plt.title(f\"Random Forest Distribution of Station-wise R² Scores for {target_pollutant}\", fontsize=14, weight='bold', pad=15)\n",
    "        plt.xlabel(\"R² Score\", fontsize=12)\n",
    "        plt.ylabel(\"Number of Stations\", fontsize=12)\n",
    "\n",
    "        # Grid styling\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Legend and layout\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"rf_Hist_test_Station_R²_Scores_for_{target_pollutant}\")\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # Residual Analysis\n",
    "        residuals = y_test - y_test_pred\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title(f'Random Forest Residual Distribution for {target_pollutant}')\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'RF_Residual_Distribution_for_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # plotting tree\n",
    "        tree_to_plot = regr.estimators_[0]\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plot_tree(tree_to_plot, max_depth = 3, feature_names=X_train.columns.tolist(), filled=True, rounded=True, fontsize=10)\n",
    "        plt.title(f\"Decision Tree from Random Forest for {target_pollutant}\")\n",
    "        plt.savefig(f\"Decision_Tree_from_RF_for_{target_pollutant}.png\")\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "    if plot_graphs == True:\n",
    "        plotting()\n",
    "\n",
    "    return regr, y_test_pred, r2_test, rmse_test, nrmse_test, pearson_test, spearman_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b53a4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_pol = 'O3'\n",
    "#X_feature_columns = [f for f in all_feature_columns if f != target_pol]\n",
    "#X_train, X_test, y_train, y_test, scaler_y = preprocess_subset(df, all_feature_columns, target_pollutant= target_pol, splitting_method = 'by_station')\n",
    "#model = random_forest(X_train, X_test, y_train, y_test, feature_columns= X_feature_columns, target_pollutant = 'O3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f916fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_time(X_train, X_test, y_train, y_test, regr= None, feature_columns = None, print_res = True, plot_graphs = True, show_figs = False, target_pollutant = 'O3'):\n",
    "\n",
    "\n",
    "    if feature_columns is not None:\n",
    "        X_train = X_train[feature_columns]\n",
    "        X_test = X_test[feature_columns]\n",
    "\n",
    "    # Then create and fit model ONLY on training data\n",
    "    if regr == None:\n",
    "\n",
    "        regr = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state= 42,\n",
    "            n_jobs= -1\n",
    "        )\n",
    "\n",
    "        regr.fit(X_train.drop(columns=['datetime']), y_train.drop(columns=['stationid_int']))\n",
    "\n",
    "    \n",
    "    else: \n",
    "\n",
    "        regr = RandomForestRegressor(**regr.get_params())\n",
    "         # Fit directly with NaN values\n",
    "        regr.fit(X_train.drop(columns=['datetime']), y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = regr.predict(X_train.drop(columns=['datetime']))\n",
    "    y_test_pred = regr.predict(X_test.drop(columns=['datetime']))\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate(y_true, y_pred, print_result, dataset_type=\"Test\"):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        nrmse = rmse / y_true.mean()  \n",
    "        \n",
    "        # --- Correlations ---\n",
    "        pearson_r, _ = pearsonr(y_true, y_pred)\n",
    "        spearman_r, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "        if print_result == True:\n",
    "            print(f\"{dataset_type} MAE: {mae:.2f}\")\n",
    "            print(f\"{dataset_type} MSE: {mse:.2f}\")\n",
    "            print(f\"{dataset_type} RMSE: {rmse:.2f}\")\n",
    "            print(f\"{dataset_type} R²: {r2:.2f}\")\n",
    "            print(f\"{dataset_type} NRMSE: {nrmse:.2f}\")\n",
    "            print(f\"{dataset_type} Person Corr: {pearson_r:.2f}\")\n",
    "            print(f\"{dataset_type} Spearman Corr: {spearman_r:.2f}\")\n",
    "            \n",
    "        return r2, rmse, nrmse, pearson_r, spearman_r\n",
    "\n",
    "    r2_train, rmse_train, nrmse_train, pearson_train, spearman_train = evaluate(y_train, y_train_pred, print_res, \"Train\")\n",
    "    r2_test, rmse_test, nrmse_test, pearson_test, spearman_test = evaluate(y_test, y_test_pred, print_res, \"Test\")\n",
    "\n",
    "    def plotting():\n",
    "\n",
    "        # Feature Importance\n",
    "        importances = regr.feature_importances_\n",
    "        feature_names = X_train.drop(columns=['datetime']).columns\n",
    "        feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "        # plot feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "        plt.title(f'Feature Importances for {target_pollutant} from Random Forest')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Feature_importances_rf_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "        \n",
    "        random_station = np.random.choice(X_test['stationid_int'].unique())\n",
    "\n",
    "\n",
    "        mask = X_test['stationid_int'] == random_station\n",
    "        X_test_station = X_test[mask]\n",
    "        y_test_station = y_test[mask.values]\n",
    "        y_pred_station = y_test_pred[mask.values]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(X_test_station['datetime'], y_test_station, label=f'Station = {random_station} Actual {target_pollutant}',  alpha=0.7)\n",
    "        plt.plot(X_test_station['datetime'], y_pred_station, label=f'Station = {random_station} Predicted {target_pollutant}', alpha=0.7)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(f\"{target_pollutant} Levels\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.title(f'{target_pollutant} Time Series: Actual vs Predicted with RF')\n",
    "        plt.savefig(f'rf_timeseries_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "          # Combine everything into one DataFrame for convenience\n",
    "        df_results = pd.DataFrame({\n",
    "            'stationid_int': X_test['stationid_int'].values,\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_test_pred\n",
    "        })\n",
    "\n",
    "        # Compute R² for each station\n",
    "        station_r2 = (\n",
    "            df_results.groupby('stationid_int', group_keys=False)\n",
    "            .apply(lambda g: r2_score(g['y_true'], g['y_pred']), include_groups=False)\n",
    "        )\n",
    "\n",
    "        # Print top/bottom performing stations\n",
    "        print(\"Top 5 stations by R²:\")\n",
    "        print(station_r2.sort_values(ascending=False).head())\n",
    "        print(\"\\nBottom 5 stations by R²:\")\n",
    "        print(station_r2.sort_values().head())\n",
    "\n",
    "        plt.figure(figsize=(9, 5))\n",
    "        n, bins, patches = plt.hist(\n",
    "            station_r2, bins=8, color='#4C72B0', edgecolor='white', alpha=0.8, rwidth=0.9\n",
    "        )\n",
    "\n",
    "        plt.axvline(r2_test, color='red', linestyle='--', linewidth=2, label=f'Overall = {r2_test:.2f}')\n",
    "\n",
    "        # Titles and labels\n",
    "        plt.title(f\"Random Forest Distribution of Station-wise R² Scores for {target_pollutant}\", fontsize=14, weight='bold', pad=15)\n",
    "        plt.xlabel(\"R² Score\", fontsize=12)\n",
    "        plt.ylabel(\"Number of Stations\", fontsize=12)\n",
    "\n",
    "        # Grid styling\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Legend and layout\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"rf_Hist_test_Station_R²_Scores_for_{target_pollutant}\")\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # Residual Analysis\n",
    "        residuals = y_test - y_test_pred\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title(f'Random Forest Residual Distribution for {target_pollutant}')\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'RF_Residual_Distribution_for_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # plotting tree\n",
    "        tree_to_plot = regr.estimators_[0]\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plot_tree(tree_to_plot, max_depth = 3, feature_names=X_train.columns.tolist(), filled=True, rounded=True, fontsize=10)\n",
    "        plt.title(f\"Decision Tree from Random Forest for {target_pollutant}\")\n",
    "        plt.savefig(f\"Decision_Tree_from_RF_for_{target_pollutant}.png\")\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "    if plot_graphs == True:\n",
    "        plotting()\n",
    "\n",
    "    return regr, y_test_pred, r2_test, rmse_test, nrmse_test, pearson_test, spearman_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d300bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_pol = 'O3'\n",
    "#X_feature_columns = [f for f in all_feature_columns if f != target_pol]\n",
    "#X_train, X_test, y_train, y_test, scaler_y = preprocess(df, all_feature_columns, target_pollutant= target_pol, splitting_method = 'random')\n",
    "#model = random_forest_time(X_train, X_test, y_train, y_test, feature_columns= X_feature_columns, target_pollutant = 'O3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c7ea0",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62691a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rfecv_xgb(X_train, y_train, X_features, protected_features= [], target_pollutant = 'O3', step=1, cv=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom RFECV that protects some features from elimination\n",
    "    and stores feature importances at each step.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : sklearn estimator\n",
    "    X_train : pd.DataFrame\n",
    "    y_train : pd.Series or array\n",
    "    protected_features : list of column names to never eliminate\n",
    "    step : int, number of features to remove per iteration\n",
    "    cv : int, cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_features : list of selected features\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating validation set for perutaition importances\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = data_splitting(train_df, X_features, target_pollutant)\n",
    "\n",
    "    if protected_features is None:\n",
    "        protected_features = []\n",
    "\n",
    "    X_work = X_train_split.drop(columns=['stationid_int', 'datetime']).copy()\n",
    "    X_work_val = X_val_split.drop(columns=['stationid_int', 'datetime']).copy()\n",
    "    features_remaining = list(X_work.columns)\n",
    "    results = []\n",
    "    vali_scores = []\n",
    "    num_pollutants_list = []\n",
    "\n",
    "    step_counter = 0\n",
    "\n",
    "    while len([f for f in features_remaining if f not in protected_features]) >= step:\n",
    "\n",
    "        num_pollutants_list.append(len([f for f in features_remaining if f not in protected_features]))\n",
    "        \n",
    "        print(f'Testing with {len([f for f in features_remaining if f not in protected_features])} features')\n",
    "\n",
    "        estimator = HistGradientBoostingRegressor(\n",
    "            random_state=42,\n",
    "            early_stopping = True,\n",
    "            n_iter_no_change = 10,\n",
    "            max_iter = 1000)\n",
    "            \n",
    "        # Compute CV score on current feature set\n",
    "        y_pred = cross_val_predict(estimator, X_work[features_remaining], y_train_split, cv=cv)\n",
    "        mse = mean_squared_error(y_train_split, y_pred)\n",
    "        vali_scores.append(mse)\n",
    "        \n",
    "        # Fit estimator\n",
    "        estimator.fit(X_work[features_remaining], y_train_split)\n",
    "\n",
    "        # Calculate permutation importance on test set\n",
    "        perm_importance = permutation_importance(estimator, X_work_val[features_remaining], y_val_split, random_state=42, n_repeats=5)\n",
    "\n",
    "        importances = pd.DataFrame({\n",
    "                'feature': features_remaining,\n",
    "                'importance': perm_importance.importances_mean\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            'remaining_features': features_remaining.copy(),\n",
    "            'importances': importances\n",
    "        })\n",
    "\n",
    "        # Identify candidates for elimination\n",
    "        candidates = [f for f in features_remaining if f not in protected_features]\n",
    "        n_remove = min(step, len(candidates))\n",
    "        lowest_features = importances[importances['feature'].isin(candidates)].tail(n_remove)['feature'].tolist()\n",
    "\n",
    "        # Remove lowest importance features\n",
    "        for f in lowest_features:\n",
    "            features_remaining.remove(f)\n",
    "\n",
    "        # Optional: stop when number of features = number of protected features\n",
    "        if len([f for f in features_remaining if f not in protected_features]) <= 0:\n",
    "            break\n",
    "\n",
    "    best_performance = min(vali_scores)\n",
    "    best_idx = vali_scores.index(min(vali_scores))\n",
    "    best_num_features = num_pollutants_list[best_idx]\n",
    "\n",
    "    \n",
    "    threshold = best_performance * 1.10  # 10% worse than best\n",
    "    close_indices = [i for i, v in enumerate(vali_scores) if v <= threshold]\n",
    "\n",
    "    if close_indices:\n",
    "        # pick the LAST step within threshold → fewest features\n",
    "        min_idx_within_10pct = max(close_indices)\n",
    "        min_features_within_10pct = num_pollutants_list[min_idx_within_10pct]\n",
    "    else:\n",
    "        # fallback to the true best if none qualify\n",
    "        min_idx_within_10pct = best_idx\n",
    "        min_features_within_10pct = best_num_features\n",
    "\n",
    "    print(f\"Optimal number of features: {best_num_features} (MSE={best_performance:.4f})\")\n",
    "    print(f\"Smallest feature count within 10% of best: {min_features_within_10pct} (Step {min_idx_within_10pct})\")\n",
    "\n",
    "\n",
    "    print(f\"Optimal number of features: {best_num_features} (MSE={best_performance:.4f})\")\n",
    "    print(f\"Smallest feature count within 10% of best: {min_features_within_10pct} (Step {min_idx_within_10pct})\")\n",
    "\n",
    "    history_df = pd.concat([step['importances'].assign(step=i) for i, step in enumerate(results)], ignore_index=True)\n",
    "\n",
    "    # Extract features from history_df at the best step\n",
    "    optimal_step_df = history_df[history_df[\"step\"] == best_idx]\n",
    "    optimal_features = optimal_step_df[\"feature\"].tolist()\n",
    "\n",
    "    print(f\"Optimal features: {optimal_features}\")\n",
    "\n",
    "    plt.plot(num_pollutants_list, vali_scores, marker='o', label='Validation MSE')\n",
    "    plt.axhline(threshold, color='r', linestyle='--', label='10% threshold')\n",
    "    plt.axvline(best_num_features, color='g', linestyle='--', label='Optimal')\n",
    "    plt.axvline(min_features_within_10pct, color='orange', linestyle='--', label='Within 10%')\n",
    "    plt.xlabel(\"Number of Features\")\n",
    "    plt.ylabel(\"Validation MSE\")\n",
    "    plt.title(f\"HXGBoost Elimination Curve for {y_train.name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'HXGBoost_Elimination_Curve_for_{y_train.name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_features, results[min_idx_within_10pct]['remaining_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "322c31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_pol = 'NOx'\n",
    "#X_feature_columns = [f for f in all_feature_columns if f != target_pol]\n",
    "#X_train, X_test, y_train, y_test, scaler_y = preprocess_subset(df, all_feature_columns, target_pollutant= target_pol, scaler = True)\n",
    "#optimal_features_xgb = custom_rfecv_xgb(X_train, y_train, X_feature_columns,target_pollutant= target_pol,step=1,cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84b319ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(selected, ranked)\n",
    "# top 5 features\n",
    "#hxb_important = ranked['Feature'].head(9).tolist()\n",
    "#print(hxb_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d621955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_grid_search(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    param_grid = {\n",
    "    'max_leaf_nodes' : [31], \n",
    "    'max_features': [0.6, 0.8, 1.0],\n",
    "    'max_depth': [10, 25, None],\n",
    "    'learning_rate' : [0.05, 0.01]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator= HistGradientBoostingRegressor(max_iter=1000, random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose =3,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train.drop(columns=['stationid_int', 'datetime']), y_train)\n",
    "\n",
    "    print(\"Best Parameters:\", grid.best_params_)\n",
    "    print(\"Best Score:\", -grid.best_score_)\n",
    "\n",
    "    return grid.best_params_, grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b82d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params_xgb, best_model_xgb = xgb_grid_search(X_train_sub, X_test_sub, y_train_sub, y_test_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e86f1f",
   "metadata": {},
   "source": [
    "Plateu of performance after 9 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7480f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hxg_boost(X_train, X_test, y_train, y_test, selected_features = None, print_res = True, plot_graphs = True, show_figs = False, optim_model = None, target_pollutant = 'O3'):\n",
    "\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "\n",
    "    if optim_model == None:\n",
    "        # Then create and fit model ONLY on training data\n",
    "        model = HistGradientBoostingRegressor(\n",
    "            random_state=42,\n",
    "            early_stopping = True,\n",
    "            n_iter_no_change = 100,\n",
    "            max_iter = 5000\n",
    "        )\n",
    "\n",
    "        # Fit directly with NaN values\n",
    "        model.fit(X_train.drop(columns=['stationid_int', 'datetime']), y_train)\n",
    "\n",
    "    else: \n",
    "\n",
    "        model = HistGradientBoostingRegressor(\n",
    "        **{**optim_model.get_params(), \"max_iter\": 5000})\n",
    "         # Fit directly with NaN values\n",
    "        model.fit(X_train.drop(columns=['stationid_int', 'datetime']), y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test.drop(columns=['stationid_int', 'datetime']))\n",
    "    \n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    nrmse = rmse/ y_test.mean()\n",
    "    # --- Correlations ---\n",
    "    pearson_r, _ = pearsonr(y_test, y_pred)\n",
    "    spearman_r, _ = spearmanr(y_test, y_pred)\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f'R²: {r2:.3f}')\n",
    "        print(f'RMSE: {rmse:.3f}')\n",
    "        print(f'NRMSE: {nrmse:.3f}')\n",
    "        print(f'MAE: {mae:.3f}')\n",
    "        print(f'Pearson Corr: {pearson_r:.3f}')\n",
    "        print(f'Spearman Corr: {spearman_r:.3f}')\n",
    "\n",
    "\n",
    "    def plotting():\n",
    "\n",
    "        # --- 4️⃣ Extract training and validation scores ---\n",
    "        train_scores = -np.array(model.train_score_)  # convert from neg MSE to positive MSE\n",
    "        val_scores = -np.array(model.validation_score_) if model.validation_score_ is not None else None\n",
    "\n",
    "        # --- 5️⃣ Plot learning curves ---\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(train_scores, label=\"Training MSE\", marker='o', alpha=0.7)\n",
    "        if val_scores is not None:\n",
    "            plt.plot(val_scores, label=\"Validation MSE\", marker='s', alpha=0.7)\n",
    "        plt.xlabel(\"Boosting Iterations\")\n",
    "        plt.ylabel(\"Mean Squared Error\")\n",
    "        plt.title(f\"HGB Learning Curves for {target_pollutant}\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"HGB_Learning_Curves_for_{target_pollutant}.png\")\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # Combine everything into one DataFrame for convenience\n",
    "        df_results = pd.DataFrame({\n",
    "            'stationid_int': X_test['stationid_int'].values,\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred\n",
    "        })\n",
    "\n",
    "        # Compute R² for each station\n",
    "        station_r2 = (\n",
    "            df_results.groupby('stationid_int', group_keys=False)\n",
    "            .apply(lambda g: r2_score(g['y_true'], g['y_pred']), include_groups=False)\n",
    "        )\n",
    "\n",
    "        # Print top/bottom performing stations\n",
    "        print(\"Top 5 stations by R²:\")\n",
    "        print(station_r2.sort_values(ascending=False).head())\n",
    "        print(\"\\nBottom 5 stations by R²:\")\n",
    "        print(station_r2.sort_values().head())\n",
    "\n",
    "        plt.figure(figsize=(9, 5))\n",
    "        n, bins, patches = plt.hist(\n",
    "            station_r2, bins=8, color='#4C72B0', edgecolor='white', alpha=0.8, rwidth=0.9\n",
    "        )\n",
    "\n",
    "        plt.axvline(r2, color='red', linestyle='--', linewidth=2, label=f'Overall = {r2:.2f}')\n",
    "\n",
    "        # Titles and labels\n",
    "        plt.title(f\"HGB Distribution of Station-wise R² for {target_pollutant}\", fontsize=14, weight='bold', pad=15)\n",
    "        plt.xlabel(\"R² Score\", fontsize=12)\n",
    "        plt.ylabel(\"Number of Stations\", fontsize=12)\n",
    "\n",
    "        # Grid styling\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Legend and layout\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"HGB_test_Distribution _R²_{target_pollutant}.png\")\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "        \n",
    "        random_station = np.random.choice(X_test['stationid_int'].unique())\n",
    "\n",
    "        mask = X_test['stationid_int'] == random_station\n",
    "        X_test_station = X_test.loc[mask]\n",
    "        y_test_station = y_test[mask.values]\n",
    "        y_pred_station = y_pred[mask.values]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_test_station[-100:], label=f'Station = {random_station} Actual {target_pollutant}',  alpha=0.7)\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_pred_station[-100:], label=f'Station = {random_station} Predicted {target_pollutant}', alpha=0.7)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(f\"{target_pollutant} Levels\")\n",
    "        plt.title(f'{target_pollutant} Time Series: Actual vs Predicted with HGB')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.title()\n",
    "        plt.savefig(f'HGB_timeseries_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # Calculate permutation importance on test set\n",
    "        perm_importance = permutation_importance(model, X_test.drop(columns=['stationid_int', 'datetime']), y_test, random_state=42, n_repeats=5)\n",
    "        importances = pd.Series(perm_importance.importances_mean, index=X_train.drop(columns=['stationid_int', 'datetime']).columns).sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\nPermutation Feature Importances:\")\n",
    "        print(importances)\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=importances.values, y=importances.index)\n",
    "        plt.title(f'Feature Importance for {target_pollutant} from HGB')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.savefig(f'Feature_importances_HGB_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # Residual plot\n",
    "        residuals = y_test - y_pred\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title(f'HGB Residual Distribution for {target_pollutant}')\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'RF_Residual_Distribution_for_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    if plot_graphs == True:\n",
    "        plotting()\n",
    "\n",
    "    return model, y_pred, r2, rmse, nrmse, pearson_r, spearman_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94289eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change target polluatant if wanting to predict for different pollutant.\n",
    "#X_train, X_test, y_train, y_test, scaler_y = preprocess(df, all_feature_columns, target_pollutant= 'NOx', scaler = False)\n",
    "#X_feature_columns = [f for f in all_feature_columns if f != 'NOx']\n",
    "#xgb = hxg_boost(X_train, X_test, y_train, y_test, selected_features = X_feature_columns, target_pollutant = 'NOx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fedff",
   "metadata": {},
   "source": [
    "Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "290b0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_reg(X_test_rf, X_test_xgb, y_test, rf_model, xgb_model, print_res = True):\n",
    "\n",
    "    # Get predictions from both models\n",
    "    y_pred_rf = rf_model.predict(X_test_rf.drop(columns=['stationid_int', 'datetime']))\n",
    "    y_pred_xgb = xgb_model.predict(X_test_xgb.drop(columns=['stationid_int', 'datetime']))\n",
    "\n",
    "    # Simple unweighted average (equal voting)\n",
    "    y_pred = (y_pred_rf + y_pred_xgb) / 2\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    nrmse = rmse/ y_test.mean()\n",
    "    # --- Correlations ---\n",
    "    pearson_r, _ = pearsonr(y_test, y_pred)\n",
    "    spearman_r, _ = spearmanr(y_test, y_pred)\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f'R²: {r2:.3f}')\n",
    "        print(f'RMSE: {rmse:.3f}')\n",
    "        print(f'NRMSE: {nrmse:.3f}')\n",
    "        print(f'MAE: {mae:.3f}')\n",
    "        print(f'Pearson Corr: {pearson_r:.3f}')\n",
    "        print(f'Spearman Corr: {spearman_r:.3f}')\n",
    "\n",
    "    return y_pred, r2, rmse, nrmse, pearson_r, spearman_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f09bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pollutant(df, features, subsection = None):\n",
    "\n",
    "    # list of pollutants\n",
    "    pollutants = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']# 'Benzene','Toluene','Xylene']\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['Pollutant', 'Model', 'R2', 'RMSE', 'NRMSE', 'Pearson Corr', 'Spearman Corr'])\n",
    "\n",
    "    for target_pollutant in pollutants:\n",
    "\n",
    "        print(f'Predicting: {target_pollutant}')\n",
    "\n",
    "        X_features = [col for col in features if col != target_pollutant]\n",
    "\n",
    "        if subsection == None:\n",
    "            # Change target polluatant if wanting to predict for different pollutant.\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess(df, features, target_pollutant)\n",
    "\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess_subset(df, features, target_pollutant, subset_size= subsection)\n",
    "\n",
    "        rf_model, rf_y_pred, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf = random_forest(X_train, X_test, y_train, y_test,\n",
    "                                               feature_columns= X_features, print_res = False, plot_graphs = False) \n",
    "\n",
    "        results_df.loc[len(results_df)] = [target_pollutant, 'RandomForest', r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf]\n",
    "\n",
    "        xgb_model, xgb_y_pred, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb = hxg_boost(X_train,  X_test, y_train, y_test, selected_features= X_features, print_res= False, plot_graphs = False) \n",
    "\n",
    "        results_df.loc[len(results_df)] = [target_pollutant, 'HistXGboost', r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb]\n",
    "\n",
    "        voting_y_pred, r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote = voting_reg(X_test, X_test, y_test, rf_model, xgb_model, print_res = False)\n",
    "        \n",
    "        results_df.loc[len(results_df)] = [target_pollutant, 'Voting Regressor', r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote]\n",
    "\n",
    "    print(results_df)\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    axes = axes.flatten()  # flatten 2D axes array for easy indexing\n",
    "\n",
    "    # --- R² Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='R2', hue='Model', ax=axes[0],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[0].set_title('R² Comparison by Pollutant', fontsize=14)\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_xlabel('Pollutant')\n",
    "    axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].legend(title='Model')\n",
    "\n",
    "    # --- NRMSE Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='NRMSE', hue='Model', ax=axes[1],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[1].set_title('NRMSE (mean-normalized) Comparison by Pollutant', fontsize=14)\n",
    "    axes[1].set_ylabel('NRMSE (mean-normalized)')\n",
    "    axes[1].set_xlabel('Pollutant')\n",
    "    axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].legend(title='Model')\n",
    "\n",
    "    # --- Pearson r Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='Pearson Corr', hue='Model', ax=axes[2],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[2].set_title('Pearson Correlation by Pollutant', fontsize=14)\n",
    "    axes[2].set_ylabel('Pearson r')\n",
    "    axes[2].set_xlabel('Pollutant')\n",
    "    axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].legend(title='Model')\n",
    "\n",
    "    # --- Spearman r Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='Spearman Corr', hue='Model', ax=axes[3],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[3].set_title('Spearman Correlation by Pollutant', fontsize=14)\n",
    "    axes[3].set_ylabel('Spearman r')\n",
    "    axes[3].set_xlabel('Pollutant')\n",
    "    axes[3].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[3].tick_params(axis='x', rotation=45)\n",
    "    axes[3].legend(title='Model')\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_pollutants_metric_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "    return results_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d47c5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all features\n",
    "#all_pollutant_results_df = all_pollutant(df, all_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0516119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_pollutant_results_df.to_csv('all_pollutant_results_df.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9577394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pollutant_cv(df, features, subsection=None, n_simulations=5):\n",
    "    \"\"\"\n",
    "    Run multiple independent simulations (random splits each time)\n",
    "    for each pollutant using custom preprocess functions.\n",
    "    \"\"\"\n",
    "\n",
    "    pollutants = [\n",
    "        'PM2.5','PM10', 'NO','NO2', 'NOx', 'NH3',\n",
    "        'CO', 'SO2', 'O3']#, 'Benzene','Toluene','Xylene'\n",
    "    #]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'Pollutant', 'Model',\n",
    "        'R2_mean', 'R2_std',\n",
    "        'RMSE_mean', 'RMSE_std',\n",
    "        'NRMSE_mean', 'NRMSE_std',\n",
    "        'Pearson_mean', 'Pearson_std',\n",
    "        'Spearman_mean', 'Spearman_std'\n",
    "    ])\n",
    "\n",
    "    for target_pollutant in pollutants:\n",
    "        print(f\"Running {n_simulations} simulations for pollutant: {target_pollutant}\")\n",
    "\n",
    "        X_features = [col for col in features if col != target_pollutant]\n",
    "\n",
    "        # Collect metrics across simulations\n",
    "        sim_metrics = {\n",
    "            'RandomForest': {'r2': [], 'rmse': [], 'nrmse': [], 'pearson': [], 'spearman': []},\n",
    "            'HistXGBoost': {'r2': [], 'rmse': [], 'nrmse': [], 'pearson': [], 'spearman': []},\n",
    "            'Voting Regressor': {'r2': [], 'rmse': [], 'nrmse': [], 'pearson': [], 'spearman': []}\n",
    "        }\n",
    "\n",
    "        for sim in range(1, n_simulations + 1):\n",
    "            print(f\"  Simulation {sim}/{n_simulations}...\")\n",
    "\n",
    "            # Call your existing preprocessing function\n",
    "            if subsection is None:\n",
    "                X_train, X_test, y_train, y_test, y_scaler = preprocess(\n",
    "                    df, features, target_pollutant\n",
    "                )\n",
    "            else:\n",
    "                X_train, X_test, y_train, y_test, y_scaler = preprocess_subset(\n",
    "                    df, features, target_pollutant, subset_size=subsection\n",
    "                )\n",
    "\n",
    "            # --- Random Forest ---\n",
    "            rf_model, rf_y_pred, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf = random_forest(\n",
    "                X_train, X_test, y_train, y_test,\n",
    "                feature_columns=X_features, print_res=False, plot_graphs=False\n",
    "            )\n",
    "\n",
    "            # --- XGBoost ---\n",
    "            xgb_model, xgb_y_pred, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb = hxg_boost(\n",
    "                X_train, X_test, y_train, y_test,\n",
    "                selected_features=X_features, print_res=False, plot_graphs=False\n",
    "            )\n",
    "\n",
    "            # --- Voting Regressor ---\n",
    "            voting_y_pred, r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote = voting_reg(\n",
    "                X_test, X_test, y_test, rf_model, xgb_model, print_res=False\n",
    "            )\n",
    "\n",
    "            # Store results for each simulation\n",
    "            sim_metrics['RandomForest']['r2'].append(r2_rf)\n",
    "            sim_metrics['RandomForest']['rmse'].append(rmse_rf)\n",
    "            sim_metrics['RandomForest']['nrmse'].append(nrmse_rf)\n",
    "            sim_metrics['RandomForest']['pearson'].append(pearson_rf)\n",
    "            sim_metrics['RandomForest']['spearman'].append(spearman_rf)\n",
    "\n",
    "            sim_metrics['HistXGBoost']['r2'].append(r2_xgb)\n",
    "            sim_metrics['HistXGBoost']['rmse'].append(rmse_xgb)\n",
    "            sim_metrics['HistXGBoost']['nrmse'].append(nrmse_xgb)\n",
    "            sim_metrics['HistXGBoost']['pearson'].append(pearson_xgb)\n",
    "            sim_metrics['HistXGBoost']['spearman'].append(spearman_xgb)\n",
    "\n",
    "            sim_metrics['Voting Regressor']['r2'].append(r2_vote)\n",
    "            sim_metrics['Voting Regressor']['rmse'].append(rmse_vote)\n",
    "            sim_metrics['Voting Regressor']['nrmse'].append(nrmse_vote)\n",
    "            sim_metrics['Voting Regressor']['pearson'].append(pearson_vote)\n",
    "            sim_metrics['Voting Regressor']['spearman'].append(spearman_vote)\n",
    "\n",
    "        # Aggregate results across simulations\n",
    "        for model_name, vals in sim_metrics.items():\n",
    "            results_df.loc[len(results_df)] = [\n",
    "                target_pollutant, model_name,\n",
    "                np.mean(vals['r2']), np.std(vals['r2']),\n",
    "                np.mean(vals['rmse']), np.std(vals['rmse']),\n",
    "                np.mean(vals['nrmse']), np.std(vals['nrmse']),\n",
    "                np.mean(vals['pearson']), np.std(vals['pearson']),\n",
    "                np.mean(vals['spearman']), np.std(vals['spearman'])\n",
    "            ]\n",
    "\n",
    "    # --- Visualization with error bars ---\n",
    "    metrics = [\n",
    "        ('R2', 'R² across simulations ± std'),\n",
    "        ('NRMSE', 'NRMSE across simulations ± std'),\n",
    "        ('Pearson', 'Pearson correlation (mean ± std)'),\n",
    "        ('Spearman', 'Spearman correlation (mean ± std)')\n",
    "    ]\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, (metric, title) in zip(axes, metrics):\n",
    "        mean_col = f'{metric}_mean'\n",
    "        std_col = f'{metric}_std'\n",
    "        \n",
    "        # --- Bar plot (without seaborn's built-in error bars) ---\n",
    "        sns.barplot(\n",
    "            data=results_df, x='Pollutant', y=mean_col, hue='Model',\n",
    "            ax=ax, edgecolor='black', linewidth=1.2, errorbar=None\n",
    "        )\n",
    "\n",
    "        # --- Manually add error bars ---\n",
    "        # Compute bar positions manually from grouped data\n",
    "        for i, pollutant in enumerate(results_df['Pollutant'].unique()):\n",
    "            subset = results_df[results_df['Pollutant'] == pollutant]\n",
    "            n_models = subset['Model'].nunique()\n",
    "            bar_width = 0.8 / n_models  # matches seaborn default grouping\n",
    "            for j, (_, row) in enumerate(subset.iterrows()):\n",
    "                x_pos = i - 0.4 + (j + 0.5) * bar_width\n",
    "                ax.errorbar(\n",
    "                    x=x_pos, y=row[mean_col], \n",
    "                    yerr=row[std_col],\n",
    "                    fmt='none', ecolor='black', elinewidth=1.2, capsize=3, capthick=1\n",
    "                )\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_pollutants_cv_metrics.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c029399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_pollutant_cv_df = all_pollutant_cv(df, all_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d92598fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_pollutant_cv_df.to_csv('all_pollutant_cv_results_df.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b5181e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_stations_results_hist(df, all_feature_columns, target_pol='O3', n_splits=5, rf_params = None, rf_best_features = None, xgb_params = None, xgb_best_features = None, show_figs = False, reduced_feat = False, best_and_worst = True):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1️⃣ Station-based data splitting into fixed folds\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def data_splitting_fixed_folds(df, X_feature_columns, target_pollutant='O3', n_splits=5, seed=42):\n",
    "        df_split = df.copy()\n",
    "        stations = df['stationid_int'].unique()\n",
    "\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(stations)\n",
    "\n",
    "        station_folds = np.array_split(stations, n_splits)\n",
    "        folds = []\n",
    "\n",
    "        for i in range(n_splits):\n",
    "            test_stations = station_folds[i]\n",
    "            train_stations = np.concatenate([station_folds[j] for j in range(n_splits) if j != i])\n",
    "\n",
    "            train_set = df_split[df_split['stationid_int'].isin(train_stations)]\n",
    "            test_set = df_split[df_split['stationid_int'].isin(test_stations)]\n",
    "\n",
    "            X_train, X_test = train_set[X_feature_columns], test_set[X_feature_columns]\n",
    "            y_train, y_test = train_set[target_pollutant], test_set[target_pollutant]\n",
    "\n",
    "            folds.append((X_train, X_test, y_train, y_test))\n",
    "        return folds\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2️⃣ Preprocessing (drop NaNs, remove outliers, optionally scale)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def preprocess_folds(df, feature_columns, target_pollutant='O3', scaler=False, n_splits=5, seed = 42):\n",
    "        X_features = [col for col in feature_columns if col != target_pollutant]\n",
    "        df_no_Na = df.dropna(subset=[target_pollutant])\n",
    "        df_clean = removing_outlier_in_target(df_no_Na, target_pollutant)\n",
    "\n",
    "        if scaler:\n",
    "            df_norm, scaler_y = norm(df_clean, X_features, target_pollutant)\n",
    "        else:\n",
    "            df_norm, scaler_y = df_clean, None\n",
    "\n",
    "        folds = data_splitting_fixed_folds(df_norm, X_features, target_pollutant, n_splits, seed)\n",
    "        return folds\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3️⃣ Cross-validation across station folds\n",
    "    # -------------------------------------------------------------------------\n",
    "    folds = preprocess_folds(df, all_feature_columns, target_pol, seed = 10)\n",
    "    all_results_rf, all_results_xgb = [], []\n",
    "\n",
    "    if rf_params != None:\n",
    "        rf_model_best = RandomForestRegressor(**rf_params)\n",
    "    else:\n",
    "        rf_model_best = None\n",
    "\n",
    "    if xgb_params != None:\n",
    "        xgb_model_best = HistGradientBoostingRegressor(**xgb_params)\n",
    "    else:\n",
    "        xgb_model_best = None\n",
    "\n",
    "    for fold_idx, (X_train, X_test, y_train, y_test) in enumerate(folds, 1):#\n",
    "\n",
    "\n",
    "        print(f\"\\n--- Fold {fold_idx}/{n_splits} ---\")\n",
    "        print(f\"Testing on {len(X_test['stationid_int'].unique())} stations\")\n",
    "\n",
    "        X_features = [col for col in all_feature_columns if col != target_pol]\n",
    "\n",
    "        if rf_best_features != None:\n",
    "            \n",
    "            X_train_rf, X_test_rf = X_train[rf_best_features], X_test[rf_best_features]\n",
    "\n",
    "        if xgb_best_features != None:\n",
    "            \n",
    "            X_train_xgb, X_test_xgb = X_train[xgb_best_features], X_test[xgb_best_features]\n",
    "\n",
    "        # --- Random Forest ---\n",
    "        rf_model, rf_y_pred, *_ = random_forest(\n",
    "            X_train_rf, X_test_rf, y_train, y_test,\n",
    "            feature_columns=rf_best_features, regr = rf_model_best, print_res=False, plot_graphs=False\n",
    "        )\n",
    "        fold_rf = pd.DataFrame({\n",
    "            'stationid_int': X_test['stationid_int'].values,\n",
    "            'y_true': y_test.values,\n",
    "            'y_pred': rf_y_pred,\n",
    "            'model': 'RandomForest',\n",
    "            'datetime': X_test['datetime'].values\n",
    "        })\n",
    "        all_results_rf.append(fold_rf)\n",
    "\n",
    "        # --- XGBoost ---\n",
    "        xgb_model, xgb_y_pred, *_ = hxg_boost(\n",
    "            X_train_xgb, X_test_xgb, y_train, y_test,\n",
    "            selected_features=xgb_best_features, optim_model= xgb_model_best ,print_res=False, plot_graphs=False\n",
    "        )\n",
    "        fold_xgb = pd.DataFrame({\n",
    "            'stationid_int': X_test['stationid_int'].values,\n",
    "            'y_true': y_test.values,\n",
    "            'y_pred': xgb_y_pred,\n",
    "            'model': 'XGBoost',\n",
    "            'datetime': X_test['datetime'].values\n",
    "        })\n",
    "        all_results_xgb.append(fold_xgb)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4️⃣ Combine all folds and compute per-station R²\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_results = pd.concat(all_results_rf + all_results_xgb, ignore_index=True)\n",
    "\n",
    "    station_r2 = (\n",
    "        df_results.groupby(['model', 'stationid_int'], group_keys=False)\n",
    "        .apply(lambda g: r2_score(g['y_true'], g['y_pred']), include_groups=False)\n",
    "        .reset_index(name='R2')\n",
    "    )\n",
    "\n",
    "    overall_r2 = (\n",
    "        df_results.groupby('model')\n",
    "        .apply(lambda g: r2_score(g['y_true'], g['y_pred']), include_groups=False)\n",
    "    )\n",
    "\n",
    "     \n",
    "    if reduced_feat == True:\n",
    "        print(f\"Overall R² per model for {target_pol} with Reduced Features:\")\n",
    "        print(overall_r2)\n",
    "\n",
    "    else:\n",
    "        print(f\"Overall R² per model for {target_pol} with Reduced Features:\")\n",
    "        print(overall_r2)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5️⃣ Plot histograms of station-wise R²\n",
    "    # -------------------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "    models = ['RandomForest', 'XGBoost']\n",
    "    colors = ['#4C72B0', '#55A868']\n",
    "\n",
    "    for ax, model, color in zip(axes, models, colors):\n",
    "        model_r2 = station_r2[station_r2['model'] == model]['R2']\n",
    "        overall = overall_r2[model]\n",
    "\n",
    "        ax.hist(model_r2, bins=20, color=color, edgecolor='white', alpha=0.85, rwidth=0.9)\n",
    "        ax.axvline(overall, color='red', linestyle='--', linewidth=2, label=f'Overall = {overall:.2f}')\n",
    "        if reduced_feat == True:\n",
    "            ax.set_title(f\"{model}: Station-wise R² Distribution for {target_pol} using Reduced Features\", fontsize=14, weight='bold', pad=10)\n",
    "        else:\n",
    "            ax.set_title(f\"{model}: Station-wise R² Distribution for {target_pol}\", fontsize=14, weight='bold', pad=10)\n",
    "        ax.set_xlabel(\"R² Score\", fontsize=12)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        ax.legend()\n",
    "\n",
    "    axes[0].set_ylabel(\"Number of Stations\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    if reduced_feat == True:\n",
    "        plt.savefig(f'hist_all_stations_for_{target_pol}_using_Reduced_Features.png')\n",
    "    else:\n",
    "        plt.savefig(f'hist_all_stations_for_{target_pol}.png')\n",
    "\n",
    "    if show_figs == True:\n",
    "        plt.show()\n",
    "\n",
    "    if best_and_worst == True:\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 6️⃣ Print 10 best and 10 worst stations for each model\n",
    "        # -------------------------------------------------------------------------\n",
    "        for model in models:\n",
    "            print(f\"\\n📈 Top 10 Stations for {model}:\")\n",
    "            print(station_r2[station_r2['model'] == model].sort_values('R2', ascending=False).head(10))\n",
    "\n",
    "            print(f\"\\n📉 Worst 10 Stations for {model}:\")\n",
    "            print(station_r2[station_r2['model'] == model].sort_values('R2', ascending=True).head(10))\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # 7️⃣ Plot time series for the *worst-performing* station of each model\n",
    "        # -------------------------------------------------------------------------\n",
    "\n",
    "        worst_stations = (\n",
    "            station_r2.sort_values(by='R2')\n",
    "            .groupby('model', as_index=False)\n",
    "            .first()\n",
    "        )\n",
    "\n",
    "        print(\"Worst-performing stations by model:\")\n",
    "        print(worst_stations)\n",
    "\n",
    "        for _, row in worst_stations.iterrows():\n",
    "            model = row['model']\n",
    "            station_id = row['stationid_int']\n",
    "            r2_val = row['R2']\n",
    "\n",
    "            df_station = df_results[\n",
    "                (df_results['model'] == model) &\n",
    "                (df_results['stationid_int'] == station_id)\n",
    "            ].copy()\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(df_station['datetime'], df_station['y_true'], label='Actual')\n",
    "            plt.plot(df_station['datetime'], df_station['y_pred'], label=f'{model} Prediction', linestyle='--', alpha=0.7)\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(f\"{target_pol} Levels\")\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.title(f\"{model}: {target_pol} at Worst Station {station_id} (R² = {r2_val:.2f})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{model}:_{target_pol}_timeseries_pred_at_Worst Station_{station_id}.png\")\n",
    "\n",
    "            if show_figs == True:\n",
    "                plt.show()\n",
    "\n",
    "            df_station_corr_worst = df[df['stationid_int'] == station_id].copy()\n",
    "            df_station_corr_worst = df_station_corr_worst[X_features]\n",
    "            df_station_corr_worst = df_station_corr_worst.drop(['stationid_int', 'datetime'], axis = 1)\n",
    "\n",
    "            corr_matrix = df_station_corr_worst.corr()\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.title(f'Correlation Matrix for worst station {station_id} for {target_pol}')\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "            plt.savefig('corr_matrix_worst_station_for_{target_pol}.png')\n",
    "\n",
    "            if show_figs == True:\n",
    "                plt.show()\n",
    "\n",
    "        best_stations = (\n",
    "        station_r2.sort_values(by='R2')\n",
    "        .groupby('model', as_index=False,)\n",
    "        .last()\n",
    "        )\n",
    "\n",
    "        print(\"Best-performing stations by model:\")\n",
    "        print(best_stations)\n",
    "\n",
    "        for _, row in best_stations.iterrows():\n",
    "            model = row['model']\n",
    "            station_id = row['stationid_int']\n",
    "            r2_val = row['R2']\n",
    "\n",
    "            df_station = df_results[\n",
    "                (df_results['model'] == model) &\n",
    "                (df_results['stationid_int'] == station_id)\n",
    "            ].copy()\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(df_station['datetime'], df_station['y_true'], label='Actual')\n",
    "            plt.plot(df_station['datetime'], df_station['y_pred'], label=f'{model} Prediction', linestyle='--', alpha=0.7)\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(f\"{target_pol} Levels\")\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.title(f\"{model}: {target_pol} at Best Station {station_id} (R² = {r2_val:.2f})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{model}:_{target_pol}_timeseries_pred_at_Worst Station_{station_id}.png\")\n",
    "            if show_figs == True:\n",
    "                plt.show()\n",
    "\n",
    "            df_station_corr_best = df[df['stationid_int'] == station_id].copy()\n",
    "            df_station_corr_best = df_station_corr_best[X_features]\n",
    "            df_station_corr_best = df_station_corr_best.drop(['stationid_int', 'datetime'], axis = 1)\n",
    "            corr_matrix = df_station_corr_best.corr()\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.title(f'Correlation Matrix for best station {station_id}')\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "            plt.savefig('corr_matrix_best_station_for_{target_pol}.png')\n",
    "\n",
    "            if show_figs == True:\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "    return df_results, station_r2, overall_r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab7589fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for model in hist_res_r2[\\'model\\'].unique():\\n    print(f\"\\n===== {model} =====\")\\n\\n    model_r2 = hist_res_r2[hist_res_r2[\\'model\\'] == model].copy()\\n    model_r2_sorted = model_r2.sort_values(by=\\'R2\\', ascending=False)\\n\\n    print(\"\\nTop 10 stations by R²:\")\\n    print(model_r2_sorted.head(10).to_string(index=False))\\n\\n    print(\"\\nBottom 10 stations by R²:\")\\n    print(model_r2_sorted.tail(10).to_string(index=False))\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hist_results, hist_res_r2, hist_res_overall_r2 = all_stations_results_hist(df, all_feature_columns, 'NOx')\n",
    "\n",
    "# --- Top & Bottom 10 stations per model ---\n",
    "\"\"\"for model in hist_res_r2['model'].unique():\n",
    "    print(f\"\\n===== {model} =====\")\n",
    "\n",
    "    model_r2 = hist_res_r2[hist_res_r2['model'] == model].copy()\n",
    "    model_r2_sorted = model_r2.sort_values(by='R2', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 stations by R²:\")\n",
    "    print(model_r2_sorted.head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\nBottom 10 stations by R²:\")\n",
    "    print(model_r2_sorted.tail(10).to_string(index=False))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482d942",
   "metadata": {},
   "source": [
    "Full pipeline for a pollutant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4bae79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(df, target_pollutant, feature_columns, subsection = None, plot_graphs = True, show_figs = False):\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # Split data into train and test\n",
    "    print('Splitting Dataset')\n",
    "    # get target feature\n",
    "    X_features = [col for col in feature_columns if col != target_pollutant]\n",
    "\n",
    "    if subsection == None:\n",
    "        # Change target polluatant if wanting to predict for different pollutant.\n",
    "        X_train, X_test, y_train, y_test, y_scaler = preprocess(df, feature_columns, target_pollutant)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test, y_scaler = preprocess_subset(df, feature_columns, target_pollutant, subset_size= subsection)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # feature optimisation \n",
    "\n",
    "    print('Feature optimisation for Random Forest:')\n",
    "    optimal_features_rf, reduced_feat_rf = custom_rfecv_rf(X_train, y_train, step=1, cv=5)\n",
    "    print('Feature optimisation for HXGB')\n",
    "    optimal_features_xgb, reduced_feat_xgb = custom_rfecv_xgb(X_train, y_train, X_features, step=1, cv=5, target_pollutant= target_pollutant)\n",
    "\n",
    "\n",
    "    # Need to ensure station is still tracked\n",
    "    optimal_features_rf.append('stationid_int')\n",
    "    optimal_features_rf.append('datetime')\n",
    "    optimal_features_xgb.append('stationid_int')\n",
    "    optimal_features_xgb.append('datetime')\n",
    "    X_train_rf, X_test_rf = X_train[optimal_features_rf], X_test[optimal_features_rf]\n",
    "    X_train_xgb, X_test_xgb = X_train[optimal_features_xgb],  X_test[optimal_features_xgb]\n",
    "\n",
    "    reduced_feat_rf.append('stationid_int')\n",
    "    reduced_feat_rf.append('datetime')\n",
    "    reduced_feat_xgb.append('stationid_int')\n",
    "    reduced_feat_xgb.append('datetime')\n",
    "    X_train_rf_red, X_test_rf_red = X_train[reduced_feat_rf], X_test[reduced_feat_rf]\n",
    "    X_train_xgb_red, X_test_xgb_red = X_train[reduced_feat_xgb],  X_test[reduced_feat_xgb]\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Grid search parmeters for selected features to predict with\n",
    "    print('Grid searching best parameters for Random Forest:')\n",
    "    print('Testing Optimal Features:')\n",
    "    best_params_rf, best_model_rf = random_forest_grid_search(X_train_rf, X_test_rf, y_train, y_test)\n",
    "    print('Testing Reduced Features')\n",
    "    reduced_params_rf, reduced_model_rf = random_forest_grid_search(X_train_rf_red, X_test_rf_red, y_train, y_test)\n",
    "    print('Grid searching best parameters for HXGB')\n",
    "    print('Testing Optimal Features:')\n",
    "    best_params_xgb, best_model_xgb = xgb_grid_search(X_train_xgb, X_test_xgb, y_train, y_test)\n",
    "    print('Testing Reduced Features')\n",
    "    reduced_params_xgb, reduced_model_xgb = xgb_grid_search(X_train_xgb_red, X_test_xgb_red, y_train, y_test)\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    # predicting with optimised models\n",
    "\n",
    "    print('Predicting with optimised models on Test Data:')\n",
    "    results_df = pd.DataFrame(columns=['Pollutant', 'Model', 'Feature Optimisation' , 'R2', 'RMSE', 'NRMSE', 'Pearson Corr', 'Spearman Corr'])\n",
    "\n",
    "\n",
    "    rf_model, rf_y_pred, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf = random_forest(X_train_rf, X_test_rf, y_train, y_test,\n",
    "                                            feature_columns= optimal_features_rf, print_res = False, plot_graphs = False, regr = best_model_rf, target_pollutant = target_pollutant) \n",
    "    \n",
    "    rf_model_red, rf_y_pred_red, r2_rf_red, rmse_rf_red, nrmse_rf_red, pearson_rf_red, spearman_rf_red = random_forest(X_train_rf_red, X_test_rf_red, y_train, y_test,\n",
    "                                            feature_columns= reduced_feat_rf, print_res = False, plot_graphs = False, regr = reduced_model_rf, target_pollutant = target_pollutant) \n",
    "\n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'RandomForest', 'Optimal', r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf]\n",
    "\n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'RandomForest', 'Reduced', r2_rf_red, rmse_rf_red, nrmse_rf_red, pearson_rf_red, spearman_rf_red]\n",
    "\n",
    "    xgb_model, xgb_y_pred, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb = hxg_boost(X_train_xgb,  X_test_xgb, y_train, y_test, selected_features= optimal_features_xgb,\n",
    "                                                                                   print_res= False, plot_graphs = False, optim_model = best_model_xgb, target_pollutant = target_pollutant) \n",
    "    \n",
    "    \n",
    "    xgb_model_red, xgb_y_pred_red, r2_xgb_red, rmse_xgb_red, nrmse_xgb_red, pearson_xgb_red, spearman_xgb_red = hxg_boost(X_train_xgb_red,  X_test_xgb_red, y_train, y_test, selected_features= reduced_feat_xgb,\n",
    "                                                                                   print_res= False, plot_graphs = False, optim_model = reduced_model_xgb, target_pollutant = target_pollutant) \n",
    "\n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'HistXGboost', 'Optimal', r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb]\n",
    "\n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'HistXGboost', 'Reduced', r2_xgb_red, rmse_xgb_red, nrmse_xgb_red, pearson_xgb_red, spearman_xgb_red]\n",
    "\n",
    "    voting_y_pred, r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote = voting_reg(X_test_rf, X_test_xgb, y_test, best_model_rf, best_model_xgb, print_res = False)\n",
    "\n",
    "    \n",
    "    voting_y_pred_red, r2_vote_red, rmse_vote_red, nrmse_vote_red, pearson_vote_red, spearman_vote_red = voting_reg(X_test_rf_red, X_test_xgb_red, y_test, reduced_model_rf, reduced_model_xgb, print_res = False)\n",
    "    \n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'Voting Regressor', 'Optimal', r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote]\n",
    "\n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'Voting Regressor', 'Reduced', r2_vote_red, rmse_vote_red, nrmse_vote_red, pearson_vote_red, spearman_vote_red]\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # plotting results\n",
    "    if plot_graphs == True:\n",
    "\n",
    "        sns.set_style(\"whitegrid\")\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        axes = axes.flatten()  # flatten 2D axes array for easy indexing\n",
    "\n",
    "        # --- R² Plot ---\n",
    "        sns.barplot(data=results_df, x='Pollutant', y='R2', hue='Model', ax=axes[0],\n",
    "                    edgecolor='black', linewidth=1.2)\n",
    "        axes[0].set_title(f'R² for {target_pollutant} by Regression Model', fontsize=14)\n",
    "        axes[0].set_ylabel('R² Score')\n",
    "        axes[0].set_xlabel('Pollutant')\n",
    "        axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].legend(title='Model')\n",
    "\n",
    "        # --- NRMSE Plot ---\n",
    "        sns.barplot(data=results_df, x='Pollutant', y='RMSE', hue='Model', ax=axes[1],\n",
    "                    edgecolor='black', linewidth=1.2)\n",
    "        axes[1].set_title(f'RMSE for {target_pollutant} by Regression Model', fontsize=14)\n",
    "        axes[1].set_ylabel('RMSE')\n",
    "        axes[1].set_xlabel('Pollutant')\n",
    "        axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].legend(title='Model')\n",
    "\n",
    "        # --- Pearson r Plot ---\n",
    "        sns.barplot(data=results_df, x='Pollutant', y='Pearson Corr', hue='Model', ax=axes[2],\n",
    "                    edgecolor='black', linewidth=1.2)\n",
    "        axes[2].set_title(f'Pearson Correlation for {target_pollutant} by Regression Model', fontsize=14)\n",
    "        axes[2].set_ylabel('Pearson r')\n",
    "        axes[2].set_xlabel('Pollutant')\n",
    "        axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        axes[2].legend(title='Model')\n",
    "\n",
    "        # --- Spearman r Plot ---\n",
    "        sns.barplot(data=results_df, x='Pollutant', y='Spearman Corr', hue='Model', ax=axes[3],\n",
    "                    edgecolor='black', linewidth=1.2)\n",
    "        axes[3].set_title(f'Spearman Correlation for {target_pollutant} by Regression Model', fontsize=14)\n",
    "        axes[3].set_ylabel('Spearman r')\n",
    "        axes[3].set_xlabel('Pollutant')\n",
    "        axes[3].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        axes[3].tick_params(axis='x', rotation=45)\n",
    "        axes[3].legend(title='Model')\n",
    "\n",
    "        # Adjust layout for better spacing\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'optimised_metrics_for_{target_pollutant}.png')\n",
    "\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        random_station = np.random.choice(X_test['stationid_int'].unique())\n",
    "\n",
    "        mask = X_test['stationid_int'] == random_station\n",
    "        X_test_station = X_test.loc[mask]\n",
    "        y_test_station = y_test[mask.values]\n",
    "        rf_y_pred_station = rf_y_pred[mask.values]\n",
    "        xgb_y_pred_station = xgb_y_pred[mask.values]\n",
    "        voting_y_pred_station = voting_y_pred[mask.values]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_test_station[-100:], label=f'Actual {target_pollutant}')\n",
    "        plt.plot(X_test_station['datetime'][-100:], rf_y_pred_station[-100:], label=f'RF Prediction', alpha=0.7, linestyle='--')\n",
    "        plt.plot(X_test_station['datetime'][-100:], xgb_y_pred_station[-100:], label=f'XGB Prediction', alpha=0.7,linestyle='--')\n",
    "        plt.plot(X_test_station['datetime'][-100:], voting_y_pred_station[-100:], label=f'Voting Prediction', alpha=0.7, linestyle='--')\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(f\"{target_pollutant} Levels\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.title(f'{target_pollutant} Timeseries at Random Test Station {random_station}: Actual VS Predicted')\n",
    "        plt.savefig(f'Timeseries_predicion_pipline_for_{target_pollutant}.png')\n",
    "        if show_figs == True:\n",
    "            plt.show()\n",
    "\n",
    "    # testing best params and best features across all stations\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    print('Testing optimised parameters across all stations:')\n",
    "\n",
    "    hist_results_optimal, hist_res_r2_optimal, hist_res_overall_r2_optimal = all_stations_results_hist(df, all_feature_columns, target_pol = target_pollutant,\n",
    "                                                                                rf_params= best_params_rf, rf_best_features= optimal_features_rf,\n",
    "                                                                                xgb_params = best_params_xgb, xgb_best_features = optimal_features_xgb, best_and_worst= False)\n",
    "    \n",
    "    print('Testing reduced feature parameters across all stations:')\n",
    "    \n",
    "    hist_results_reduced, hist_res_r2_reduced, hist_res_overall_r2_reduced = all_stations_results_hist(df, all_feature_columns, target_pol = target_pollutant,\n",
    "                                                                                rf_params= reduced_params_rf, rf_best_features= reduced_feat_rf,\n",
    "                                                                                xgb_params = reduced_params_xgb, xgb_best_features = reduced_feat_xgb,\n",
    "                                                                                reduced_feat= True, best_and_worst= False)\n",
    "\n",
    "\n",
    "    return results_df, best_model_rf, optimal_features_rf, best_params_rf,  best_model_xgb, optimal_features_xgb, best_params_xgb, reduced_model_rf, reduced_feat_rf, reduced_params_rf,  reduced_model_xgb, reduced_feat_xgb, reduced_params_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0ceb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all features\n",
    "#optimal_rf_model, optimal_features_rf, optimal_params_rf, optimal_xgb_model, optimal_features_xgb, optimal_params_xgb ,optimisation_results_df = full_pipeline(df, 'NOx', all_feature_columns)\n",
    "#optimisation_results_df.to_csv('Results_NOx_weather.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93901163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_all_pollutants(df, pollutant_list, feature_columns, subsection=None):\n",
    "    \"\"\"\n",
    "    Runs full_pipeline() for multiple pollutants and aggregates:\n",
    "      - all performance metrics (combined_results_df)\n",
    "      - all model configurations (best_config_df)\n",
    "    \"\"\"\n",
    "\n",
    "    all_results = []       # Collects results_df from each pollutant\n",
    "    all_best_params = []   # Collects parameter/feature info from each model variant\n",
    "\n",
    "    for pollutant in pollutant_list:\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\"Running pipeline for {pollutant}\")\n",
    "        print(f\"============================\")\n",
    "\n",
    "        (\n",
    "            results_df,\n",
    "            best_model_rf, optimal_features_rf, best_params_rf,\n",
    "            best_model_xgb, optimal_features_xgb, best_params_xgb,\n",
    "            reduced_model_rf, reduced_feat_rf, reduced_params_rf,\n",
    "            reduced_model_xgb, reduced_feat_xgb, reduced_params_xgb\n",
    "        ) = full_pipeline(df, pollutant, feature_columns, subsection=subsection, plot_graphs=True)\n",
    "\n",
    "        # Store model performance results\n",
    "        all_results.append(results_df)\n",
    "\n",
    "        # Store model configurations — both optimal and reduced\n",
    "        all_best_params.extend([\n",
    "            {\n",
    "                'Pollutant': pollutant,\n",
    "                'Model': 'RandomForest',\n",
    "                'Feature Optimisation': 'Optimal',\n",
    "                'Best_Params': best_params_rf,\n",
    "                'Selected_Features': optimal_features_rf\n",
    "            },\n",
    "            {\n",
    "                'Pollutant': pollutant,\n",
    "                'Model': 'RandomForest',\n",
    "                'Feature Optimisation': 'Reduced (≤10% MSE)',\n",
    "                'Best_Params': reduced_params_rf,\n",
    "                'Selected_Features': reduced_feat_rf\n",
    "            },\n",
    "            {\n",
    "                'Pollutant': pollutant,\n",
    "                'Model': 'HistXGBoost',\n",
    "                'Feature Optimisation': 'Optimal',\n",
    "                'Best_Params': best_params_xgb,\n",
    "                'Selected_Features': optimal_features_xgb\n",
    "            },\n",
    "            {\n",
    "                'Pollutant': pollutant,\n",
    "                'Model': 'HistXGBoost',\n",
    "                'Feature Optimisation': 'Reduced (≤10% MSE)',\n",
    "                'Best_Params': reduced_params_xgb,\n",
    "                'Selected_Features': reduced_feat_xgb\n",
    "            }\n",
    "        ])\n",
    "\n",
    "    # Combine all pollutants’ results\n",
    "    combined_results_df = pd.concat(all_results, ignore_index=True)\n",
    "    best_config_df = pd.DataFrame(all_best_params)\n",
    "\n",
    "    print(\"\\n✅ All pollutants completed.\")\n",
    "    print(f\"Total pollutants processed: {len(pollutant_list)}\")\n",
    "    print(f\"Combined results shape: {combined_results_df.shape}\")\n",
    "    print(f\"Combined config shape: {best_config_df.shape}\")\n",
    "\n",
    "    return combined_results_df, best_config_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a160881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Running pipeline for NOx\n",
      "============================\n",
      "Splitting Dataset\n",
      "Feature optimisation for Random Forest:\n",
      "Testing with 16 features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m pollutants = [\u001b[33m'\u001b[39m\u001b[33mNOx\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mO3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPM10\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNO\u001b[39m\u001b[33m'\u001b[39m] \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m optimised_all_results_df, best_configerations_df = \u001b[43mpipeline_all_pollutants\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpollutants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_feature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsection\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m optimised_all_results_df.to_csv(\u001b[33m'\u001b[39m\u001b[33moptimised_all_pollutant_results_df.csv\u001b[39m\u001b[33m'\u001b[39m, index= \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m best_configerations_df.to_csv(\u001b[33m'\u001b[39m\u001b[33mbest_config_results_df.csv\u001b[39m\u001b[33m'\u001b[39m, index= \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mpipeline_all_pollutants\u001b[39m\u001b[34m(df, pollutant_list, feature_columns, subsection)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning pipeline for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpollutant\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m============================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m (\n\u001b[32m     17\u001b[39m     results_df,\n\u001b[32m     18\u001b[39m     best_model_rf, optimal_features_rf, best_params_rf,\n\u001b[32m     19\u001b[39m     best_model_xgb, optimal_features_xgb, best_params_xgb,\n\u001b[32m     20\u001b[39m     reduced_model_rf, reduced_feat_rf, reduced_params_rf,\n\u001b[32m     21\u001b[39m     reduced_model_xgb, reduced_feat_xgb, reduced_params_xgb\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m ) = \u001b[43mfull_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpollutant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsection\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_graphs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Store model performance results\u001b[39;00m\n\u001b[32m     25\u001b[39m all_results.append(results_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mfull_pipeline\u001b[39m\u001b[34m(df, target_pollutant, feature_columns, subsection, plot_graphs, show_figs)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# feature optimisation \u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFeature optimisation for Random Forest:\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m optimal_features_rf, reduced_feat_rf = \u001b[43mcustom_rfecv_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFeature optimisation for HXGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m optimal_features_xgb, reduced_feat_xgb = custom_rfecv_xgb(X_train, y_train, X_features, step=\u001b[32m1\u001b[39m, cv=\u001b[32m5\u001b[39m, target_pollutant= target_pollutant)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mcustom_rfecv_rf\u001b[39m\u001b[34m(X_train, y_train, protected_features, step, cv)\u001b[39m\n\u001b[32m     37\u001b[39m estimator = RandomForestRegressor(\n\u001b[32m     38\u001b[39m     n_estimators=\u001b[32m100\u001b[39m,\n\u001b[32m     39\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Compute CV score on current feature set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m y_pred = \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_work\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_remaining\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m mse = mean_squared_error(y_train, y_pred)\n\u001b[32m     45\u001b[39m vali_scores.append(mse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1234\u001b[39m, in \u001b[36mcross_val_predict\u001b[39m\u001b[34m(estimator, X, y, groups, cv, n_jobs, verbose, params, pre_dispatch, method)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m   1233\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m predictions = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1247\u001b[39m inv_test_indices = np.empty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m   1248\u001b[39m inv_test_indices[test_indices] = np.arange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1319\u001b[39m, in \u001b[36m_fit_and_predict\u001b[39m\u001b[34m(estimator, X, y, train, test, fit_params, method)\u001b[39m\n\u001b[32m   1317\u001b[39m     estimator.fit(X_train, **fit_params)\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1320\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[32m   1321\u001b[39m predictions = func(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\angus\\miniconda3\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "pollutants = ['NOx', 'O3', 'PM10', 'NO'] \n",
    "\n",
    "optimised_all_results_df, best_configerations_df = pipeline_all_pollutants(df, pollutants, all_feature_columns)\n",
    "\n",
    "optimised_all_results_df.to_csv('optimised_all_pollutant_results_df.csv', index= False)\n",
    "\n",
    "best_configerations_df.to_csv('best_config_results_df.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16db3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pipeline_results(results_df, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots grouped bar charts (R², RMSE, Pearson Corr, Spearman Corr)\n",
    "    for each pollutant separately, showing model comparison and feature optimisation type.\n",
    "\n",
    "    Each pollutant gets its own subplot per metric.\n",
    "    Bars have black outlines and display their numeric values.\n",
    "    'Voting Regressor' is excluded only from the plots (not from the dataframe).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_df : pd.DataFrame\n",
    "        Columns required:\n",
    "        ['Pollutant', 'Model', 'Feature Optimisation', 'R2', 'RMSE',\n",
    "         'Pearson Corr', 'Spearman Corr']\n",
    "    save_path : str, optional\n",
    "        Directory path to save figures. If None, plots are shown interactively.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = ['R2', 'RMSE', 'Pearson Corr', 'Spearman Corr']\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    pollutants = results_df['Pollutant'].unique()\n",
    "\n",
    "    for metric in metrics:\n",
    "        n_pollutants = len(pollutants)\n",
    "        fig, axes = plt.subplots(\n",
    "            1, n_pollutants, figsize=(6 * n_pollutants, 5), sharey=False\n",
    "        )\n",
    "\n",
    "        if n_pollutants == 1:\n",
    "            axes = [axes]  # ensure iterable if only one pollutant\n",
    "\n",
    "        for ax, pollutant in zip(axes, pollutants):\n",
    "            # Filter just for plotting (keep Voting Regressor in dataframe)\n",
    "            plot_df = results_df[\n",
    "                (results_df['Pollutant'] == pollutant)\n",
    "                & (~results_df['Model'].str.contains('Voting', case=False, na=False))\n",
    "            ]\n",
    "\n",
    "            barplot = sns.barplot(\n",
    "                data=plot_df,\n",
    "                x='Model',\n",
    "                y=metric,\n",
    "                hue='Feature Optimisation',\n",
    "                ax=ax,\n",
    "                dodge=True,\n",
    "                edgecolor='black',   # black outlines\n",
    "                linewidth=1.2\n",
    "            )\n",
    "\n",
    "            # Add numeric labels on bars\n",
    "            for container in barplot.containers:\n",
    "                barplot.bar_label(\n",
    "                    container,\n",
    "                    fmt='%.2f',\n",
    "                    fontsize=8,\n",
    "                    padding=2,\n",
    "                    color='black',\n",
    "                    rotation=0\n",
    "                )\n",
    "\n",
    "            ax.set_title(f'{pollutant} — {metric}', fontsize=14)\n",
    "            ax.set_xlabel('Model Type', fontsize=12)\n",
    "            ax.set_ylabel(metric, fontsize=12)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            ax.tick_params(axis='x', rotation=30)\n",
    "            ax.legend(title='Feature Optimisation', fontsize=9, title_fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            fname = f\"{save_path.rstrip('/')}/results_{metric.replace(' ', '_')}.png\"\n",
    "            plt.savefig(fname, dpi=300, bbox_inches='tight')\n",
    "            print(f\"✅ Saved: {fname}\")\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6089744",
   "metadata": {},
   "source": [
    "### comparing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64578197",
   "metadata": {},
   "source": [
    "Comparing prediction performance with removal of correlated features to see if model can still perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a35fdd",
   "metadata": {},
   "source": [
    "Removing one of the two features that have correlation greater than 0.8. \n",
    "\n",
    "The removed feature will be based on the one with fewer NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17341a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_features_to_remove(df, threshold_corr = 0.8, target_pollutant = 'O3', print_res = False):\n",
    "\n",
    "    # list of pollutants\n",
    "    pollutants = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    # getting corralation\n",
    "    corr = df[pollutants].corr()\n",
    "\n",
    "    # 2. Keep only upper triangle (exclude diagonal & duplicates)\n",
    "\n",
    "    corr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    corr = corr[corr >= threshold_corr]\n",
    "\n",
    "    # 3. Flatten into long form (pairs + correlation value)\n",
    "    corr_pairs = (\n",
    "        corr.stack()  # removes NaN automatically\n",
    "        .reset_index()\n",
    "    )\n",
    "    corr_pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "    \n",
    "    feature_to_remove_by_Nan = []\n",
    "    feature_to_remove_by_target_corr = []\n",
    "\n",
    "    # iterate through pairs of highly correlated features\n",
    "    for pair in corr_pairs.index:\n",
    "\n",
    "        f1, f2 = corr_pairs.loc[pair, ['Feature_1', 'Feature_2']]\n",
    "\n",
    "        if f1 == target_pollutant or f2 == target_pollutant:\n",
    "            continue\n",
    "\n",
    "        # Count NaN values for each feature in the original DataFrame\n",
    "        nan_f1 = df[f1].isna().sum()\n",
    "        nan_f2 = df[f2].isna().sum()\n",
    "\n",
    "        # Compare which has more missing values\n",
    "        if nan_f1 > nan_f2:\n",
    "            if print_res == True:\n",
    "                print(f\"{f1} has more NaN values ({nan_f1}) than {f2} ({nan_f2}).\")\n",
    "            feature_to_remove_by_Nan.append(f1)\n",
    "        else:\n",
    "            if print_res == True:\n",
    "                print(f\"{f2} has more NaN values ({nan_f2}) than {f1} ({nan_f1}).\")\n",
    "            feature_to_remove_by_Nan.append(f2)\n",
    "\n",
    "        # Compute correlation with target \n",
    "        corr_f1 = df[[f1, target_pollutant]].corr().iloc[0, 1]\n",
    "        corr_f2 = df[[f2, target_pollutant]].corr().iloc[0, 1]\n",
    "\n",
    "        # Compare absolute correlation with the target\n",
    "        if corr_f1 > corr_f2:\n",
    "            if print_res == True:\n",
    "                print(f\"{f2} has lower correlation ({corr_f2:.3f}) with {target_pollutant} than {f1} ({corr_f1:.3f}). Removing {f2}.\")\n",
    "            feature_to_remove_by_target_corr.append(f2)\n",
    "        else:\n",
    "            if print_res == True:\n",
    "                print(f\"{f1} has lower correlation ({corr_f1:.3f}) with {target_pollutant} than {f2} ({corr_f2:.3f}). Removing {f1}.\")\n",
    "            feature_to_remove_by_target_corr.append(f1)\n",
    "\n",
    "\n",
    "    return corr_pairs, [feature_to_remove_by_Nan, feature_to_remove_by_target_corr]\n",
    "    \n",
    "def compare_w_removed_features(df, features, subsection = None):\n",
    "\n",
    "    # list of pollutants\n",
    "    pollutants = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['Pollutant', 'Model', 'Removal_reason', 'R2', 'RMSE', 'NRMSE', 'Pearson Corr', 'Spearman Corr'])\n",
    "\n",
    "    for target_pollutant in pollutants:\n",
    "\n",
    "        print(f'Predicting: {target_pollutant}')\n",
    "\n",
    "        if subsection == None:\n",
    "            # Change target polluatant if wanting to predict for different pollutant.\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess(df, features, target_pollutant)\n",
    "\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess_subset(df, features, target_pollutant, subset_size= subsection)\n",
    "\n",
    "        # get info on highly correlated pairs \n",
    "        corr_pairs, features_to_remove = corr_features_to_remove(df, target_pollutant = target_pollutant, print_res = False)\n",
    "        \n",
    "        # Add a baseline \"no removal\" condition\n",
    "        features_to_remove.insert(0, [])  \n",
    "        removal_methods = ['None', 'Number of NaN', 'Corr w Target']\n",
    "\n",
    "        for i, method in enumerate(removal_methods):\n",
    "            \n",
    "            remove_list = features_to_remove[i]\n",
    "            \n",
    "            X_features = [col for col in features \n",
    "              if col != target_pollutant and col not in remove_list]\n",
    "            \n",
    "\n",
    "\n",
    "            X_train_iter, X_test_iter = X_train[X_features], X_test[X_features]\n",
    "\n",
    "            rf_model, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf = random_forest(X_train_iter, X_test_iter, y_train, y_test,\n",
    "                                                feature_columns= X_features, print_res = False, plot_graphs = False) \n",
    "\n",
    "            results_df.loc[len(results_df)] = [target_pollutant, 'RandomForest', method, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf]\n",
    "\n",
    "            xgb_model, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb = hxg_boost(X_train_iter,  X_test_iter, y_train, y_test, selected_features= X_features, print_res= False, plot_graphs = False) \n",
    "\n",
    "            results_df.loc[len(results_df)] = [target_pollutant, 'HistXGboost', method, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb]\n",
    "\n",
    "\n",
    "            # Dont need votign regressor\n",
    "            #r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote = voting_reg(X_test_iter, X_test_iter, y_test, rf_model, xgb_model, print_res = False)\n",
    "            \n",
    "            #results_df.loc[len(results_df)] = [target_pollutant, 'Voting Regressor', method, r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote]\n",
    "\n",
    "    return results_df\n",
    "\n",
    "#corr_pairs, feat_to_remove = corr_features_to_remove(df, print_res = True, target_pollutant= 'PM10')\n",
    "\n",
    "# use all features\n",
    "#results_remove_corr = compare_w_removed_features(df, all_feature_columns, subsection = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5510a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_remove_corr(results_df):\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=results_df,\n",
    "        x=\"Pollutant\",\n",
    "        y=\"R2\",\n",
    "        hue=\"Removal_reason\",\n",
    "        col=\"Model\",\n",
    "        kind=\"bar\",\n",
    "        height=5,\n",
    "        aspect=1.1,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    g.fig.set_dpi(150)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set_axis_labels(\"Pollutant\", \"R²\")\n",
    "    g.add_legend(title=\"Removal reason\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plotting_remove_corr(results_remove_corr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
