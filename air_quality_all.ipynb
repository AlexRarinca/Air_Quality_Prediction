{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcad1b10",
   "metadata": {},
   "source": [
    "# O3 Air Pollution Predictive Modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cdcde6",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "38213399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data frame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "\n",
    "# machine laerning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import pearsonr, spearmanr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af61d1",
   "metadata": {},
   "source": [
    "## Time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c8519",
   "metadata": {},
   "source": [
    "Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524dd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv('India_complete_day.csv', low_memory=False)\n",
    "\n",
    "# Parse datetime\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "# Extract useful time features\n",
    "#df['hour'] = df['datetime'].dt.hour\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
    "df['year'] = df['datetime'].dt.year\n",
    "# convert StationId to integer labels\n",
    "df['stationid_int'], uniques = pd.factorize(df['StationId'])\n",
    "\n",
    "\n",
    "# for one hot encoding stationid_int- was not successful\n",
    "#one_hot_df = pd.get_dummies(df, columns = ['stationid_int'])\n",
    "#one_hot_df['stationid_int'] = df['stationid_int'] \n",
    "#print(one_hot_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11436b4e",
   "metadata": {},
   "source": [
    "Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric variables\n",
    "def summary_stats(df):\n",
    "\n",
    "    # print nimber of stations\n",
    "    print(f\"Number of unique stations: {df['StationId'].nunique()}\")\n",
    "    \n",
    "    print(\"Descriptive Statistics:\")\n",
    "    # for hour\n",
    "    #df_desc = df.drop(['hour', 'month', 'dayofweek', 'datetime', 'lat', 'lon', 'day'], axis=1)\n",
    "\n",
    "    df_desc = df.drop(['month', 'dayofweek', 'datetime', 'lat', 'lon', 'day'], axis=1)\n",
    "    print(df_desc.describe())\n",
    "\n",
    "    print(\"Missing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "#summary_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def o3_stats(df):\n",
    "    # Print O3 statistics\n",
    "    print(f\"\\nO3 Statistics:\")\n",
    "    print(f\"Mean O3: {df['O3'].mean():.2f}\")\n",
    "    print(f\"Median O3: {df['O3'].median():.2f}\")\n",
    "    print(f\"Min O3: {df['O3'].min():.2f}\")\n",
    "    print(f\"Max O3: {df['O3'].max():.2f}\")\n",
    "\n",
    "#o3_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e6d4f2",
   "metadata": {},
   "source": [
    "Map visualsation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf437b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_vis(df):\n",
    "\n",
    "    # Load Natural Earth countries (1:110m resolution)\n",
    "    url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "    world = gpd.read_file(url)\n",
    "\n",
    "    # Filter for India\n",
    "    india = world[world[\"ADMIN\"] == \"India\"]\n",
    "\n",
    "\n",
    "    # grouping by city as hard to see individual points otherwise\n",
    "\n",
    "    # groups by city and takes mean of O3 values\n",
    "    df_mean = df.groupby(['City']).mean('O3').reset_index()\n",
    "    # Remove rows with NaN values in or 'O3' columns\n",
    "    df_clean = df_mean.dropna(subset=[\"O3\"])\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_clean,  # remove NaNs\n",
    "        geometry=gpd.points_from_xy(df_clean[\"lon\"], df_clean[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # --- Plot ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    india.plot(ax=ax, color=\"white\", edgecolor=\"black\")  # India boundary\n",
    "\n",
    "    # Scatter plot of O3 levels\n",
    "    gdf.plot(\n",
    "        ax=ax,\n",
    "        column=\"O3\",        # pollutant column\n",
    "        cmap=\"coolwarm\",        # color scheme\n",
    "        markersize=80,      # adjust dot size\n",
    "        alpha=0.8,\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "        \"label\": \"O₃ Concentration (µg/m³)\",   # ← your legend label\n",
    "        \"shrink\": 0.6                         # optional: smaller colorbar\n",
    "    }\n",
    "    )\n",
    "\n",
    "    plt.title(\"Scatter Map of Ozone (O₃) Levels Across India\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#map_vis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531452f",
   "metadata": {},
   "source": [
    "Visualising time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd1b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ozone_time_series(df):\n",
    "    \n",
    "    # group by day (drop the time component)\n",
    "    df_mean = (\n",
    "        df.groupby(df['datetime'].dt.date)['O3']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # rename for clarity\n",
    "    df_mean.columns = ['date', 'O3']\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_mean, x='date', y='O3')\n",
    "    plt.title('Daily Mean Ozone (O₃) Levels Across India')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('O₃ Concentration (µg/m³)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def all_pollutatants_time_series(df):\n",
    "\n",
    "    pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    monthly_mean = (\n",
    "    df.groupby(pd.Grouper(key='datetime', freq='D'))[pollutants]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "    )\n",
    "\n",
    "    # plot on different graphs for clarity\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for i, pollutant in enumerate(pollutants, 1):\n",
    "        plt.subplot(3, 3, i)\n",
    "        sns.lineplot(data=monthly_mean, x='datetime', y=pollutant)\n",
    "        plt.title(f'Daily Mean {pollutant} Levels Across India')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Concentration (µg/m³)')\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#ozone_time_series(df)\n",
    "#all_pollutatants_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403c43e",
   "metadata": {},
   "source": [
    "Random station ozone time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0044271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_station_ozone_time_series(df):\n",
    "\n",
    "    # select a random city\n",
    "    random_station = df['StationId'].sample(n=1).values[0]\n",
    "\n",
    "    df_station = df[df['StationId'] == random_station]\n",
    "\n",
    "    # group by day (drop the time component)\n",
    "    df_mean = (\n",
    "        df_station.groupby(df_station['datetime'].dt.date)['O3']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # rename for clarity\n",
    "    df_mean.columns = ['date', 'O3']\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df_mean, x='date', y='O3')\n",
    "    plt.title(f'Daily Mean Ozone (O₃) Levels in {df_station[\"City\"].values[0]} (Station {random_station}) ')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('O₃ Concentration (µg/m³)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# can return a city with no ozone data so may need to rerun to find one that does\n",
    "#random_station_ozone_time_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d3f32",
   "metadata": {},
   "source": [
    "Seasonal Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe03a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_fig(df):\n",
    "    # Extract year from datetime\n",
    "    # copy to not impact original df\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Year'] = df_copy['datetime'].dt.year\n",
    "    # # Year-over-Year O3 Trend\n",
    "    yearly_O3 = df_copy.groupby('Year')['O3'].mean().reset_index()\n",
    "\n",
    "    #print(\"\\nYearly O3 Summary:\")\n",
    "    #print(yearly_O3)\n",
    "\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.plot(yearly_O3['Year'], yearly_O3['O3'], marker='o', linewidth=2, \n",
    "             markersize=8, color='darkred')\n",
    "    plt.title('Year-over-Year O3 Trend', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.ylabel('Average O3', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(yearly_O3['Year'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def monthly_fig(df):\n",
    "    # Month-over-Month O3 Trend\n",
    "    monthly_O3 = df.groupby('month')['O3'].mean().reset_index()\n",
    "\n",
    "    #print(\"\\nMonthly O3 Summary:\")\n",
    "    #print(monthly_O3)\n",
    "\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.plot(monthly_O3['month'], monthly_O3['O3'], marker='o', linewidth=2, \n",
    "            markersize=8, color='darkred')\n",
    "    plt.title('Month-over-Month O3 Trend', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Month', fontsize=12)\n",
    "    plt.ylabel('Average O3', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(monthly_O3['month'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def day_of_week_fig(df):\n",
    "    # Day of the week O3 Trend\n",
    "    day_of_week_O3 = df.groupby('dayofweek')['O3'].mean().reset_index()\n",
    "\n",
    "    #print(\"\\nDaily O3 Summary:\")\n",
    "    #print(day_of_week_O3)\n",
    "\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    plt.plot(day_of_week_O3['dayofweek'], day_of_week_O3['O3'], marker='o', linewidth=2, \n",
    "            markersize=8, color='lightgreen')\n",
    "    plt.title('Day-Of-The-Week O3 Trend', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('DayOfWeek', fontsize=12)\n",
    "    plt.ylabel('Average O3', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(day_of_week_O3['dayofweek'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def seasonal_trends(df):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['weekday_name'] = df_copy['datetime'].dt.day_name()\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(6,5))\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        else:\n",
    "            return 'Autumn'\n",
    "\n",
    "    df_copy['season'] = df_copy['month'].apply(get_season)\n",
    "\n",
    "    seasonal_summary = df_copy.groupby('season')['O3'].mean().reset_index()\n",
    "    sns.barplot(x='season', y='O3', data=seasonal_summary, order=['Spring', 'Summer', 'Autumn', 'Winter'])\n",
    "    plt.title('Average Seasonal O₃ Levels')\n",
    "    plt.show()\n",
    "\n",
    "#seasonal_trends(df)\n",
    "#yearly_fig(df)\n",
    "#monthly_fig(df)\n",
    "#day_of_week_fig(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6d93f",
   "metadata": {},
   "source": [
    "Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8470bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms(df):\n",
    "    numeric_features = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    # Create subplots for all histograms\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    axes = axes.flatten()  # Make indexing easier\n",
    "\n",
    "    for i, col in enumerate(numeric_features):\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].axvline(df[col].median(), color='r', linestyle='--', label='Median'),\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#histograms(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14381b",
   "metadata": {},
   "source": [
    "Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a8ea2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df):\n",
    "\n",
    "    # Correlation matrix for pollutants.\n",
    "    df_corr = df.drop(['AQI_Bucket', 'City', 'datetime', 'StationId', 'StationName', 'Status', 'State', 'lat', 'lon', 'month', 'dayofweek', 'day', 'year', 'AQI', 'stationid_int'], axis=1)\n",
    "    corr_matrix = df_corr.corr()\n",
    "\n",
    "    # Show strong correlations with 03\n",
    "    if 'O3' in corr_matrix.columns:\n",
    "        O3_corr = corr_matrix['O3'].sort_values(ascending=False)\n",
    "        print(\"\\nCorrelation with O3:\")\n",
    "        print(O3_corr[1:11])\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "#correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9f310",
   "metadata": {},
   "source": [
    "Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa7fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_matrix(df):\n",
    "\n",
    "    # Correlation matrix for pollutants.\n",
    "    df_cov = df.drop(['AQI_Bucket', 'City', 'datetime', 'StationId', 'StationName', 'Status', 'State'], axis=1)\n",
    "    cov_matrix = df_cov.cov()\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(cov_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "\n",
    "#cov_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad808b28",
   "metadata": {},
   "source": [
    "Distribution of AQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0633b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aqi_plots(df):\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # 03 distribution\n",
    "    axes[0].hist(df['AQI'].dropna(), bins=50, color='blue', edgecolor='black')\n",
    "    axes[0].set_title('Distribution of AQI Values', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('AQI', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].axvline(df['AQI'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"AQI\"].mean():.2f}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # # AQI Bucket distribution\n",
    "    aqi_bucket_counts = df['AQI_Bucket'].value_counts()\n",
    "    axes[1].bar(aqi_bucket_counts.index, aqi_bucket_counts.values, color='red', edgecolor='black')\n",
    "    axes[1].set_title('Distribution of AQI Buckets', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('AQI Bucket', fontsize=12)\n",
    "    axes[1].set_ylabel('Count', fontsize=12)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#aqi_plots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226960f",
   "metadata": {},
   "source": [
    "Top 50 Worst days for O3 pollution all in same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2867d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_pollutant_days(df, target_pollutant = 'O3'):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Year'] = df_copy['datetime'].dt.year\n",
    "    # Get top 50 days with Highest O3\n",
    "    top_O3_days = df_copy.nlargest(50, target_pollutant)[['datetime', 'StationName',target_pollutant]]\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Top 50 days with Highest O3\")\n",
    "    print(\"=\" * 50)\n",
    "    print(top_O3_days.to_string(index=False))\n",
    "\n",
    "#top_pollutant_days(df, 'NO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a0eeb",
   "metadata": {},
   "source": [
    "Average Pollutant per stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "62913463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pollutant_by_station(df):\n",
    "\n",
    "    # pollutant list\n",
    "    pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    stations_mean = (\n",
    "        df.groupby('StationId')[pollutants]\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Stations by average pollutant levels\n",
    "\n",
    "    # Create subplots: 3 rows × 3 columns\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()  # make axes 1D for easy looping\n",
    "\n",
    "    # Plot each pollutant\n",
    "    for i, pollutant in enumerate(pollutants):\n",
    "        # Sort stations for that pollutant\n",
    "        sorted_data = stations_mean[pollutant].sort_values(ascending=False)\n",
    "\n",
    "        sorted_data.plot(\n",
    "            kind='barh',\n",
    "            ax=axes[i],\n",
    "            color='orange',\n",
    "            edgecolor='black'\n",
    "        )\n",
    "        axes[i].set_title(f'Stations by Average {pollutant}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel(f'Average {pollutant} (µg/m³)', fontsize=10)\n",
    "        axes[i].set_ylabel('Station', fontsize=10)\n",
    "        axes[i].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#avg_pollutant_by_station(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50214485",
   "metadata": {},
   "source": [
    "Finding outliers in target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "da319b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_avg_in_pollutant(df, target_pollutant = 'O3'):\n",
    "  pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "  stations_mean = (\n",
    "      df.groupby('StationName')[pollutants]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "  )\n",
    "  top_stations = (\n",
    "    stations_mean[['StationName', target_pollutant]]\n",
    "    .sort_values(by=target_pollutant, ascending=False)\n",
    "    .head(10)\n",
    "  )\n",
    "  print(top_stations)\n",
    "\n",
    "#find_max_avg_in_pollutant(df, target_pollutant= 'NO2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ce251",
   "metadata": {},
   "source": [
    "Nan value analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1cdd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_by_pollutant(df):\n",
    "\n",
    "    India = df.copy()\n",
    "\n",
    "    # Keep only pollutant-related columns\n",
    "    polls = ['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI']\n",
    "\n",
    "\n",
    "    miss_pct = {}\n",
    "    for c in polls:\n",
    "        total_valid = India[c].notna().sum() + India[c].isna().sum()  \n",
    "        miss_pct[c] = (India[c].isna().sum() / total_valid) * 100\n",
    "\n",
    "    miss_pct = pd.Series(miss_pct).sort_values(ascending=False)\n",
    "    print(\"Missing Percentage by Pollutant (%):\", miss_pct)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(x=miss_pct.index, y=miss_pct.values, color='seagreen')\n",
    "    plt.title(\"Missing Percentage by Pollutant (%)\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Missing Percentage (%)\")\n",
    "    plt.xlabel(\"Pollutant\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def nan_by_city(df):\n",
    "\n",
    "    India = df.copy()\n",
    "    #--------------------------------------------------------------------------------------------------------------------------------\n",
    "    # Calculate missing rate by city\n",
    "    # Keep only pollutant-related columns\n",
    "    polls = ['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI']\n",
    "    \n",
    "    city_miss = India.groupby('City')[polls].apply(lambda x: x.isnull().mean() * 100)\n",
    "    city_miss_mean = city_miss.mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "    print(\"Top 10 Cities with Highest Missing Rates (%):\")\n",
    "    print(city_miss_mean.head(10))\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    city_miss_mean.plot(kind='bar', color='salmon')\n",
    "    plt.title(\"Average Missing Rate by City (%)\")\n",
    "    plt.ylabel(\"Missing Percentage (%)\")\n",
    "    plt.xlabel(\"City\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(\n",
    "        city_miss,\n",
    "        cmap='YlGnBu',       \n",
    "        linewidths=0.3,       \n",
    "        linecolor='white',\n",
    "        cbar_kws={'label': 'Missing Rate (%)'},\n",
    "        annot=True,             \n",
    "        fmt=\".1f\",            \n",
    "        annot_kws={'size':8, 'color':'black'} \n",
    "    )\n",
    "\n",
    "    plt.title(\"City vs Pollutant Missing Rate Heatmap\", fontsize=16, fontweight='bold', pad=12)\n",
    "    plt.xlabel(\"Pollutant\", fontsize=12)\n",
    "    plt.ylabel(\"City\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def nan_by_station(df):\n",
    "\n",
    "    India = df.copy()\n",
    "    #  Calculate missing rate by monitoring station\n",
    "\n",
    "    polls = ['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','O3','AQI']\n",
    "\n",
    "    station_miss = India.groupby('StationName')[polls].apply(lambda x: x.isnull().mean() * 100)\n",
    "    station_miss['Average'] = station_miss.mean(axis=1)\n",
    "    station_miss = station_miss.sort_values('Average', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Monitoring Stations with Highest Missing Rates:\")\n",
    "    print(station_miss.head(10))\n",
    "\n",
    "def nan_percent_by_station(df):\n",
    "\n",
    "    # pollutant list\n",
    "    pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    # Calculate % of NaN values for each pollutant per station\n",
    "    stations_nan_pct = (\n",
    "        df.groupby('StationId')[pollutants]\n",
    "          .apply(lambda x: x.isna().mean() * 100)\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Create subplots: 3 rows × 3 columns\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()  # make axes 1D for easy looping\n",
    "\n",
    "    # Plot each pollutant\n",
    "    for i, pollutant in enumerate(pollutants):\n",
    "        # Sort stations by % NaN for that pollutant\n",
    "        sorted_data = stations_nan_pct.sort_values(by=pollutant, ascending=False)\n",
    "\n",
    "        axes[i].barh(sorted_data['StationId'], sorted_data[pollutant],\n",
    "                     color='skyblue', edgecolor='black')\n",
    "\n",
    "        axes[i].set_title(f'% NaN in {pollutant}', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('% Missing Values', fontsize=10)\n",
    "        axes[i].set_ylabel('Station', fontsize=10)\n",
    "        axes[i].tick_params(axis='y', labelsize=5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#nan_by_pollutant(df)\n",
    "#nan_by_city(df)\n",
    "#nan_by_station(df)\n",
    "#nan_percent_by_station(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fba408",
   "metadata": {},
   "source": [
    "imputing for nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8973d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_impute_selected_stations(df, threshold=30, n_neighbors=3):\n",
    "    \"\"\"\n",
    "    Perform KNN imputation only for stations with < threshold% missing data.\n",
    "\n",
    "    Parameters:\n",
    "        df : pd.DataFrame\n",
    "            Must contain 'StationName' and pollutant columns.\n",
    "        threshold : float\n",
    "            Maximum allowed % of missing values to apply KNN imputation.\n",
    "        n_neighbors : int\n",
    "            Number of neighbors for KNN imputer.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame : Imputed dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "    df = df.copy()\n",
    "\n",
    "    # Compute % of missing values per station across all pollutants\n",
    "    missing_pct = (\n",
    "        df.groupby('StationName')[pollutants]\n",
    "          .apply(lambda x: x.isna().mean().mean() * 100)\n",
    "    )\n",
    "\n",
    "    # Select stations with < threshold % missing\n",
    "    good_stations = missing_pct[missing_pct < threshold].index\n",
    "    bad_stations = missing_pct[missing_pct >= threshold].index\n",
    "\n",
    "    print(f\"Stations eligible for KNN imputation (<{threshold}% missing): {len(good_stations)}\")\n",
    "    print(f\"Stations excluded: {len(bad_stations)}\")\n",
    "\n",
    "    # Initialize KNN imputer\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "\n",
    "    # Separate data\n",
    "    df_good = df[df['StationName'].isin(good_stations)].copy()\n",
    "    df_bad = df[df['StationName'].isin(bad_stations)].copy()\n",
    "\n",
    "    # Apply KNN imputation to good stations only\n",
    "    df_good[pollutants] = imputer.fit_transform(df_good[pollutants])\n",
    "\n",
    "    # Combine back\n",
    "    df_imputed = pd.concat([df_good, df_bad], ignore_index=True)\n",
    "\n",
    "    return df_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16df078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_median(df, features):\n",
    "    df_copy = df.dropna(subset=['O3']).copy()\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_copy[features] = imputer.fit_transform(df_copy[features])\n",
    "    return df_copy\n",
    "\n",
    "def fill_missing_knn(df, features, n_neighbors=5):\n",
    "    df_copy = df.dropna(subset=['O3']).copy()\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_copy[features] = imputer.fit_transform(df_copy[features])\n",
    "    return df_copy\n",
    "\n",
    "def fill_missing_moving_average(df, features, window=3):\n",
    "    df_copy = df.dropna(subset=['O3']).copy()\n",
    "    for col in features:\n",
    "        df_copy[col] = df_copy[col].fillna(df_copy[col].rolling(window, min_periods=1).mean())\n",
    "    return df_copy\n",
    "\n",
    "def fill_missing_regression(df, features, max_iter=10):\n",
    "    df_copy = df.dropna(subset=['O3']).copy()\n",
    "    imputer = IterativeImputer(max_iter=max_iter, random_state=42)\n",
    "    df_copy[features] = imputer.fit_transform(df_copy[features])\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model Evaluation\n",
    "# ------------------------------------------------------------\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return r2, rmse\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main Comparison Function\n",
    "# ------------------------------------------------------------\n",
    "def compare_imputation_methods(df, feature_columns):\n",
    "\n",
    "    imputation_methods = {\n",
    "        'No Imputation': lambda x, f: x.dropna(subset=f + ['O3']),\n",
    "        'Median': fill_missing_median,\n",
    "        'KNN': fill_missing_knn,\n",
    "        'Moving Avg': fill_missing_moving_average,\n",
    "        'Regression (MICE)': fill_missing_regression\n",
    "    }\n",
    "\n",
    "    results_r2 = {'Random Forest': {}, 'XGBoost': {}}\n",
    "\n",
    "    for name, func in imputation_methods.items():\n",
    "\n",
    "        print(f\"=== Testing {name} ===\")\n",
    "        df_filled = func(df, feature_columns)\n",
    "\n",
    "        X = df_filled[feature_columns]\n",
    "        y = df_filled['O3']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators=120, random_state=42)\n",
    "        r2_rf, _ = evaluate_model(rf, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "        results_r2['Random Forest'][name] = r2_rf\n",
    "\n",
    "        xgb = HistGradientBoostingRegressor(max_iter=150, random_state=42)\n",
    "        r2_xgb, _ = evaluate_model(xgb, X_train, X_test, y_train, y_test)\n",
    "        results_r2['XGBoost'][name] = r2_xgb\n",
    "\n",
    "        print(f\"Random Forest R² = {r2_rf:.3f} | XGBoost R² = {r2_xgb:.3f}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Visualization: Advanced Style\n",
    "    # ============================================================\n",
    "    \n",
    "    # Simple, clean style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # DataFrame from results\n",
    "    r2_df = pd.DataFrame(results_r2)\n",
    "\n",
    "    # Basic bar plot\n",
    "    ax = r2_df.plot(\n",
    "        kind='bar',\n",
    "        figsize=(8, 4),\n",
    "        color=['#68b684', '#3c78d8'],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "    # Titles and labels\n",
    "    ax.set_title(\"R² Comparison — With & Without Imputation\", fontsize=13)\n",
    "    ax.set_ylabel(\"R² Score\")\n",
    "    ax.set_xlabel(\"Imputation Method\")\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    # Show values on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return r2_df\n",
    "\n",
    "# feature_columns = ['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','hour','month','dayofweek']\n",
    "#results = compare_imputation_methods(df, feature_columns, sample_n=1000000)\n",
    "#print(results)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Imputation Methods (time-series safe, no row dropping) =========\n",
    "def fill_missing_median_ts(df, feature):\n",
    "    \"\"\"Perform median imputation for a single feature (keeps all timestamps).\"\"\"\n",
    "    out = df.copy()\n",
    "    out[feature] = _ensure_numeric_series(out[feature], feature)\n",
    "    imp = SimpleImputer(strategy='median')\n",
    "    out[feature] = imp.fit_transform(out[[feature]])\n",
    "    return out\n",
    "\n",
    "def fill_missing_knn_ts(df, feature, n_neighbors=5):\n",
    "    \"\"\"Perform KNN imputation for a single feature (keeps all timestamps).\"\"\"\n",
    "    out = df.copy()\n",
    "    out[feature] = _ensure_numeric_series(out[feature], feature)\n",
    "    imp = KNNImputer(n_neighbors=n_neighbors)\n",
    "    out[feature] = imp.fit_transform(out[[feature]])\n",
    "    return out\n",
    "\n",
    "def fill_missing_moving_average_ts(df, feature, window=3):\n",
    "    \"\"\"Perform moving average imputation (rolling mean) for a single feature.\"\"\"\n",
    "    out = df.copy()\n",
    "    out[feature] = _ensure_numeric_series(out[feature], feature)\n",
    "    roll = out[feature].rolling(window=window, min_periods=1).mean()\n",
    "    out[feature] = out[feature].where(~out[feature].isna(), roll)\n",
    "    return out\n",
    "\n",
    "def fill_missing_regression_ts(df, feature, extra_features=None, max_iter=10):\n",
    "    \"\"\"\n",
    "    Perform Multiple Imputation by Chained Equations (MICE).\n",
    "    Safely imputes only the target feature, without dropping rows.\n",
    "    Automatically selects numeric columns and skips all-NaN predictors.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Columns to protect (not used as predictors)\n",
    "    protected = {'datetime', 'Date', 'StationId', 'StationName', 'City', 'State', 'Status'}\n",
    "\n",
    "    # Select numeric predictor columns\n",
    "    if extra_features is not None:\n",
    "        num_cols = [c for c in extra_features if c in out.columns]\n",
    "    else:\n",
    "        num_cols = [c for c in out.columns\n",
    "                    if (c not in protected)\n",
    "                    and np.issubdtype(out[c].dtype, np.number)]\n",
    "\n",
    "    # Ensure the target column is included\n",
    "    if feature not in num_cols:\n",
    "        num_cols.append(feature)\n",
    "\n",
    "    # Convert to numeric and drop columns that are all NaN\n",
    "    X = out[num_cols].apply(lambda s: pd.to_numeric(s, errors='coerce'))\n",
    "    X = X.dropna(axis=1, how='all')\n",
    "\n",
    "    # If the target column is lost, raise an error\n",
    "    if feature not in X.columns:\n",
    "        raise ValueError(f\"MICE failed: target column '{feature}' became all NaN.\")\n",
    "\n",
    "    # If too few valid predictors exist, fallback to median imputation\n",
    "    if X.shape[1] < 2:\n",
    "        print(f\"⚠️ Warning: only '{feature}' has valid numeric data. Falling back to median imputation.\")\n",
    "        imp = SimpleImputer(strategy='median')\n",
    "        out[feature] = imp.fit_transform(out[[feature]])\n",
    "        return out\n",
    "\n",
    "    # Perform MICE imputation\n",
    "    imp = IterativeImputer(max_iter=max_iter, random_state=42)\n",
    "    X_filled = imp.fit_transform(X)\n",
    "    X_filled = pd.DataFrame(X_filled, columns=X.columns, index=X.index)\n",
    "\n",
    "    # Replace only the target column\n",
    "    out[feature] = X_filled[feature]\n",
    "    return out\n",
    "\n",
    "# ========= Main Plotting Function =========\n",
    "def plot_imputation_timeseries(df, feature='O3', station_id=None, save_fig=True):\n",
    "    \"\"\"\n",
    "    Visualize and compare different imputation methods for a single monitoring station over time.\n",
    "    Requirements: df must contain ['StationId', 'datetime', feature].\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose the station (default: station with most missing data)\n",
    "    if station_id is None:\n",
    "        try:\n",
    "            missing_counts = df.groupby('StationId')[feature].apply(\n",
    "                lambda x: pd.to_numeric(x, errors='coerce').isna().sum()\n",
    "            )\n",
    "            station_id = missing_counts.idxmax()\n",
    "        except Exception:\n",
    "            station_id = df['StationId'].mode().iloc[0]\n",
    "\n",
    "    print(f\"Plotting imputation comparison for Station {station_id}, Feature {feature}\")\n",
    "\n",
    "    # Filter and sort data\n",
    "    df_station = df[df['StationId'] == station_id].copy()\n",
    "    if df_station.empty:\n",
    "        raise ValueError(f\"Station '{station_id}' not found in the data.\")\n",
    "    df_station.sort_values('datetime', inplace=True)\n",
    "\n",
    "    # Original series (with NaNs kept)\n",
    "    df_station[feature] = pd.to_numeric(df_station[feature], errors='coerce')\n",
    "    original = df_station[['datetime', feature]].rename(columns={feature: 'Original'})\n",
    "\n",
    "    # Apply four imputation methods\n",
    "    extra_feats = [c for c in df_station.columns\n",
    "                   if c not in {'datetime', 'Date', 'StationId', 'StationName', 'City', 'State', 'Status'}\n",
    "                   and c != feature\n",
    "                   and np.issubdtype(df_station[c].dtype, np.number)]\n",
    "\n",
    "    df_med = fill_missing_median_ts(df_station, feature)\n",
    "    df_knn = fill_missing_knn_ts(df_station, feature)\n",
    "    df_mov = fill_missing_moving_average_ts(df_station, feature)\n",
    "    df_reg = fill_missing_regression_ts(df_station, feature, extra_features=extra_feats or None)\n",
    "\n",
    "    # Align all results on the same timeline\n",
    "    merged = original.copy()\n",
    "    for name, data in [('Median', df_med), ('KNN', df_knn),\n",
    "                       ('MovingAvg', df_mov), ('Regression', df_reg)]:\n",
    "        tmp = data[['datetime', feature]].rename(columns={feature: name})\n",
    "        merged = pd.merge(merged, tmp, on='datetime', how='left')\n",
    "\n",
    "    # ===== Visualization =====\n",
    "    sns.set(style=\"whitegrid\", font=\"Arial\", font_scale=1.2)\n",
    "    plt.figure(figsize=(14, 6), dpi=300)\n",
    "\n",
    "    # Gray shaded area: missing regions\n",
    "    nan_mask = merged['Original'].isna()\n",
    "    if nan_mask.any():\n",
    "        ymin = np.nanmin(merged[['Original', 'Median', 'KNN', 'MovingAvg', 'Regression']].values)\n",
    "        ymax = np.nanmax(merged[['Original', 'Median', 'KNN', 'MovingAvg', 'Regression']].values)\n",
    "        plt.fill_between(merged['datetime'], ymin, ymax,\n",
    "                         where=nan_mask, color='gray', alpha=0.15,\n",
    "                         label='Missing Data Region', zorder=1)\n",
    "\n",
    "    # Imputed series (overlay)\n",
    "    plt.plot(merged['datetime'], merged['Median'], '--', color='#2e7d32', linewidth=2,\n",
    "             label='Median Imputation', zorder=3)\n",
    "    plt.plot(merged['datetime'], merged['KNN'], '--', color='#1565c0', linewidth=2,\n",
    "             label='KNN Imputation', zorder=3)\n",
    "    plt.plot(merged['datetime'], merged['MovingAvg'], '--', color='#f9a825', linewidth=2,\n",
    "             label='Moving Average', zorder=3)\n",
    "    plt.plot(merged['datetime'], merged['Regression'], '--', color='#7b1fa2', linewidth=2,\n",
    "             label='Multiple Regression (MICE)', zorder=3)\n",
    "\n",
    "    # Original curve (broken at NaNs)\n",
    "    plt.plot(merged['datetime'], merged['Original'], color='#303030',\n",
    "             linewidth=2.2, alpha=0.85, label='Original Data (with NaN)', zorder=4)\n",
    "\n",
    "    plt.title(f\"{feature} Imputation Comparison — Station {station_id}\",\n",
    "              fontsize=16, fontweight='bold', pad=10)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(f\"{feature} Concentration (µg/m³)\")\n",
    "    plt.xticks(rotation=28)\n",
    "    plt.legend(loc='upper right', frameon=True)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        fn = f\"imputation_comparison_{station_id}_{feature}.png\"\n",
    "        plt.savefig(fn, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✅ Saved high-resolution figure: {fn}\")\n",
    "\n",
    "    plt.show()\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ===================== Example Usage =====================\n",
    "# Make sure df contains: ['StationId', 'datetime', 'O3'] at minimum\n",
    "#merged = plot_imputation_timeseries(df, feature='O3', station_id='DL005', save_fig=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd81c7",
   "metadata": {},
   "source": [
    "With Kfold validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, scaling=True, k=5):\n",
    "    \"\"\"\n",
    "    Evaluate model using k-fold cross-validation (default k=5)\n",
    "    \"\"\"\n",
    "    if scaling:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X = np.array(X)\n",
    "\n",
    "    cv = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main Comparison Function\n",
    "# ------------------------------------------------------------\n",
    "def compare_imputation_methods(df, feature_columns, sample_n=30000):\n",
    "    df = df.sample(n=min(sample_n, len(df)), random_state=42)\n",
    "\n",
    "    imputation_methods = {\n",
    "        'No Imputation': lambda x, f: x.dropna(subset=f + ['O3']),\n",
    "        'Median': fill_missing_median,\n",
    "        'KNN': fill_missing_knn,\n",
    "        'Moving Avg': fill_missing_moving_average,\n",
    "        'Regression (MICE)': fill_missing_regression\n",
    "    }\n",
    "\n",
    "    results_r2 = {'Random Forest': {}, 'XGBoost': {}}\n",
    "\n",
    "    for name, func in imputation_methods.items():\n",
    "        print(f\"\\n=== Testing {name} ===\")\n",
    "        \n",
    "        df_filled = func(df, feature_columns)\n",
    "        X = df_filled[feature_columns]\n",
    "        y = df_filled['O3']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        rf = RandomForestRegressor(n_estimators=120, random_state=42)\n",
    "        r2_rf, std_rf = evaluate_model(rf, X, y, scaling=True, k=5)\n",
    "        results_r2['Random Forest'][name] = r2_rf\n",
    "        \n",
    "        xgb = HistGradientBoostingRegressor(max_iter=150, random_state=42)\n",
    "        r2_xgb, std_xgb = evaluate_model(xgb, X, y, scaling=False, k=5)\n",
    "        results_r2['XGBoost'][name] = r2_xgb\n",
    "        \n",
    "        print(f\"Random Forest R² = {r2_rf:.3f} ± {std_rf:.3f} | XGBoost R² = {r2_xgb:.3f} ± {std_xgb:.3f}\")\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # Visualization: Advanced Style\n",
    "    # ============================================================\n",
    "    sns.set(style=\"whitegrid\", font=\"Arial\", font_scale=1.2)\n",
    "    plt.rcParams['axes.edgecolor'] = '#333333'\n",
    "    plt.rcParams['axes.linewidth'] = 1.2\n",
    "\n",
    "    # --- Plot 1: R² Comparison (All Methods) ---\n",
    "    r2_df = pd.DataFrame(results_r2)\n",
    "    fig, ax = plt.subplots(figsize=(9,5))\n",
    "    bar_colors = ['#999999','#88c999','#66b2ff','#f9cc6c','#d48eff']\n",
    "\n",
    "    bars = r2_df.plot(kind='bar', ax=ax, color=['#68b684','#3c78d8'], width=0.75, edgecolor='black')\n",
    "    ax.set_title(\"R² Comparison — With & Without Imputation\", fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(\"R² Score\")\n",
    "    ax.set_xlabel(\"Imputation Method\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # highlight top performer\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.2f\", label_type=\"edge\", padding=3, fontsize=10)\n",
    "    plt.legend(title=\"Model\", loc='upper left', frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return r2_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Example Usage\n",
    "# ============================================================\n",
    "# df = pd.read_csv(\"India_complete.csv\", low_memory=False) \n",
    "# feature_columns = ['PM2.5','PM10','NO','NO2','NOx','NH3','CO','SO2','hour','month','dayofweek']\n",
    "#results = compare_imputation_methods(df, feature_columns)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be5101",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08750090",
   "metadata": {},
   "source": [
    "data set prep and splitting for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(df, X_feature_columns, target_pollutant = 'O3'):   \n",
    "    \"\"\"\n",
    "    Function to normalise selected features in the dataframe.\n",
    "    \"\"\"\n",
    "    # only normalise pollutant values\n",
    "    col_to_norm = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "    col_to_norm.remove(target_pollutant)\n",
    "\n",
    "    X = df[X_feature_columns]  # Can contain NaN values\n",
    "    y = df[[target_pollutant]]  # Should not contain NaN\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Normalised dataset \n",
    "    X_norm = X.copy()\n",
    "    # Scale only selected columns\n",
    "    scaler_X = preprocessing.MinMaxScaler()\n",
    "    X_norm[col_to_norm] = scaler_X.fit_transform(X[col_to_norm])\n",
    "    scaler_y = preprocessing.MinMaxScaler()\n",
    "    y_norm = scaler_y.fit_transform(y)\n",
    "\n",
    "    y_norm_df = pd.DataFrame(y_norm, columns=[target_pollutant], index=y.index)\n",
    "\n",
    "    df_norm = pd.concat([X_norm, y_norm_df], axis=1)\n",
    "\n",
    "    return df_norm, scaler_y\n",
    "\n",
    "def data_splitting(df, X_feature_columns, target_pollutant = 'O3'):\n",
    "\n",
    "    df_split = df.copy()\n",
    "    # get unique station names\n",
    "    stations = df['stationid_int'].unique()\n",
    "    # shuffle stations\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    np.random.shuffle(stations)\n",
    "    # split stations into train and test (80-20)\n",
    "    split_index = int(0.8 * len(stations))\n",
    "    train_stations = stations[:split_index]\n",
    "    test_stations = stations[split_index:]\n",
    "    # create train and test sets based on stations\n",
    "    train_set = df_split[df_split['stationid_int'].isin(train_stations)]\n",
    "    test_set = df_split[df_split['stationid_int'].isin(test_stations)]\n",
    "\n",
    "    # split into X and y\n",
    "    X_train, X_test = train_set[X_feature_columns], test_set[X_feature_columns]\n",
    "    y_train, y_test = train_set[target_pollutant], test_set[target_pollutant]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def removing_outlier_in_target(df, target_pollutant):\n",
    "\n",
    "    # Removing extreme outliers seen in avg_pollutant_per_station\n",
    "\n",
    "    if target_pollutant == 'O3':\n",
    "        \n",
    "        df_no_outlier = df[df['StationName'] != 'Punjabi Bagh, Delhi - DPCC']\n",
    "\n",
    "    if target_pollutant == 'CO': \n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Maninagar, Ahmedabad - GPCB']\n",
    "\n",
    "    if target_pollutant == 'NO':\n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Samanpura, Patna - BSPCB']\n",
    "\n",
    "    if target_pollutant == 'NOx':\n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Anand Vihar, Delhi - DPCC']\n",
    "        df_no_outlier = df_no_outlier[df_no_outlier['StationName'] != 'Samanpura, Patna - BSPCB']\n",
    "\n",
    "    if target_pollutant == 'SO2':\n",
    "\n",
    "        df_no_outlier = df[df['StationName'] != 'Maninagar, Ahmedabad - GPCB']\n",
    "\n",
    "    return df_no_outlier\n",
    "\n",
    "def preprocess(df, feature_columns, target_pollutant = 'O3', scaler = False):\n",
    "\n",
    "    X_features = [col for col in feature_columns if col != target_pollutant]\n",
    "    \n",
    "    # Remove rows where target variable (O3) is missing\n",
    "    df_no_Na = df.dropna(subset=[target_pollutant])\n",
    "\n",
    "    df_clean = removing_outlier_in_target(df_no_Na, target_pollutant)\n",
    "\n",
    "    if scaler == True:\n",
    "        # normaise fatures\n",
    "        df_norm, scaler_y = norm(df_clean, X_features, target_pollutant)\n",
    "    else:\n",
    "        df_norm = df_clean\n",
    "        scaler_y = None\n",
    "\n",
    "    # split data into train and test sets based on stations\n",
    "    X_train, X_test, y_train, y_test = data_splitting(df_norm, X_features, target_pollutant)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler_y\n",
    "\n",
    "def preprocess_subset(df, feature_columns, target_pollutant = 'O3', subset_size = 10, scaler = False):\n",
    "\n",
    "    X_features = [col for col in feature_columns if col != target_pollutant]\n",
    "\n",
    "    # Remove rows where target variable (O3) is missing\n",
    "    df_no_Na = df.dropna(subset=[target_pollutant])\n",
    "\n",
    "    df_clean = removing_outlier_in_target(df_no_Na, target_pollutant)\n",
    "\n",
    "    if scaler == True:\n",
    "        # normaise fatures\n",
    "        df_norm, scaler_y = norm(df_clean, X_features, target_pollutant)\n",
    "\n",
    "    else:\n",
    "        df_norm = df_clean\n",
    "        scaler_y = None\n",
    "\n",
    "    # Get n random stations\n",
    "    subset_stations = np.random.choice(df_norm['stationid_int'].unique(), size= subset_size, replace=False)\n",
    "\n",
    "    # Filter\n",
    "    subset_df = df_norm[df_norm['stationid_int'].isin(subset_stations)]\n",
    "\n",
    "    # split data into train and test sets based on stations\n",
    "    X_train, X_test, y_train, y_test = data_splitting(subset_df, X_features, target_pollutant)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler_y\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "# features not using the station id\n",
    "columns = df.columns.tolist()\n",
    "cols_to_drop = ['datetime', 'lat', 'lon', 'State', 'City', 'StationId', 'Status', 'AQI', 'AQI_Bucket', 'dayofweek', 'StationName']\n",
    "all_feature_columns = [c for c in columns if c not in cols_to_drop]\n",
    "\n",
    "# target pollutant for testing\n",
    "target_pol = 'NOx'\n",
    "\n",
    "# Change target polluatant if wanting to predict for different pollutant.\n",
    "X_train, X_test, y_train, y_test, scaler_y = preprocess(df, all_feature_columns, target_pollutant= target_pol, scaler = True)\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub, scaler_y = preprocess_subset(df, all_feature_columns, target_pollutant= target_pol)\n",
    "\n",
    "X_feature_columns = all_feature_columns.remove(target_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83979d",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b911898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_feature_elimination_forest(X_train, y_train):\n",
    "\n",
    "    min_features_to_select = 1  # Minimum number of features to consider\n",
    "    clf = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rfecv = RFECV(\n",
    "        estimator=clf,\n",
    "        step=1,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        min_features_to_select=min_features_to_select,\n",
    "        cv=KFold(5, shuffle=True, random_state=42),\n",
    "        n_jobs=-1,\n",
    "        verbose = 1\n",
    "    )\n",
    "    rfecv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Extract selected & ranked features\n",
    "    feature_names = X_train.columns\n",
    "    selected_features = feature_names[rfecv.support_]\n",
    "    feature_importances = rfecv.estimator_.feature_importances_\n",
    "    ranked_features = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": feature_importances,\n",
    "        \"Rank\": rfecv.ranking_\n",
    "    }).sort_values(\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    print(\"\\nSelected Features:\")\n",
    "    print(selected_features.tolist())\n",
    "\n",
    "    print(\"\\nFeature Ranking:\")\n",
    "    print(ranked_features)\n",
    "\n",
    "\n",
    "    print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "    \n",
    "    data = {\n",
    "        key: value\n",
    "        for key, value in rfecv.cv_results_.items()\n",
    "        if key in [\"n_features\", \"mean_test_score\", \"std_test_score\"]\n",
    "    }\n",
    "    cv_results = pd.DataFrame(data)\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Mean test accuracy\")\n",
    "    plt.errorbar(\n",
    "        x=cv_results[\"n_features\"],\n",
    "        y=cv_results[\"mean_test_score\"],\n",
    "        yerr=cv_results[\"std_test_score\"],\n",
    "    )\n",
    "    plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return selected_features, ranked_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected, ranked = recursive_feature_elimination_forest(X_train_sub, y_train_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "46cc614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rfecv_rf(X_train, y_train, protected_features= ['month', 'day', 'year'], step=1, cv=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom RFECV that protects some features from elimination\n",
    "    and stores feature importances at each step.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : sklearn estimator\n",
    "    X_train : pd.DataFrame\n",
    "    y_train : pd.Series or array\n",
    "    protected_features : list of column names to never eliminate\n",
    "    step : int, number of features to remove per iteration\n",
    "    cv : int, cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_features : list of selected features\n",
    "    \"\"\"\n",
    "    if protected_features is None:\n",
    "        protected_features = []\n",
    "\n",
    "    X_work = X_train.drop(columns=['stationid_int']).copy()\n",
    "    features_remaining = list(X_work.columns)\n",
    "    results = []\n",
    "    vali_scores = []\n",
    "    num_pollutants_list = []\n",
    "\n",
    "    step_counter = 0\n",
    "\n",
    "    while len([f for f in features_remaining if f not in protected_features]) >= step:\n",
    "\n",
    "        num_pollutants_list.append(len([f for f in features_remaining if f not in protected_features]))\n",
    "        \n",
    "        print(f'Testing with {len([f for f in features_remaining if f not in protected_features])} pollutants as features')\n",
    "\n",
    "        estimator = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Compute CV score on current feature set\n",
    "        y_pred = cross_val_predict(estimator, X_work[features_remaining], y_train, cv=cv)\n",
    "        mse = mean_squared_error(y_train, y_pred)\n",
    "        vali_scores.append(mse)\n",
    "        \n",
    "        # Fit estimator\n",
    "        estimator.fit(X_work[features_remaining], y_train)\n",
    "\n",
    "        # get importances\n",
    "        importances = estimator.feature_importances_\n",
    "\n",
    "        # Store results\n",
    "        imp_df = pd.DataFrame({\n",
    "            'feature': features_remaining,\n",
    "            'importance': importances,\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        results.append({\n",
    "            'remaining_features': features_remaining.copy(),\n",
    "            'importances': imp_df\n",
    "        })\n",
    "\n",
    "        # Identify candidates for elimination\n",
    "        candidates = [f for f in features_remaining if f not in protected_features]\n",
    "        n_remove = min(step, len(candidates))\n",
    "        lowest_features = imp_df[imp_df['feature'].isin(candidates)].tail(n_remove)['feature'].tolist()\n",
    "\n",
    "        # Remove lowest importance features\n",
    "        for f in lowest_features:\n",
    "            features_remaining.remove(f)\n",
    "\n",
    "        # Optional: stop when number of features = number of protected features\n",
    "        if len([f for f in features_remaining if f not in protected_features]) <= 0:\n",
    "            break\n",
    "\n",
    "    best_performance = min(vali_scores)\n",
    "    best_idx = vali_scores.index(min(vali_scores))\n",
    "    best_num_features = num_pollutants_list[best_idx]\n",
    "\n",
    "    print(f\"Optimal number of pollutants as features {best_num_features} with mse: {best_performance}\")\n",
    "\n",
    "    history_df = pd.concat([step['importances'].assign(step=i) for i, step in enumerate(results)], ignore_index=True)\n",
    "\n",
    "    # Extract features from history_df at the best step\n",
    "    optimal_step_df = history_df[history_df[\"step\"] == best_idx]\n",
    "    optimal_features = optimal_step_df[\"feature\"].tolist()\n",
    "\n",
    "    print(f\"Optimal features: {optimal_features}\")\n",
    "\n",
    "    plt.plot(num_pollutants_list, vali_scores, marker='o')\n",
    "    plt.xlabel(\"Number of pollutant features\")\n",
    "    plt.ylabel(\"Validation MSE\")\n",
    "    plt.title(\"RFECV Elimination Curve (Random Forest)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aef180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#protected = ['month', 'day', 'year']  # features to protect\n",
    "#optimal_features_rf = custom_rfecv_rf(X_train, y_train, protected_features=protected,step=1,cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e4e26598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_grid_search(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    param_grid = {\n",
    "    'n_estimators' : [100, 250], \n",
    "    'max_features': [0.8, 1],\n",
    "    'max_samples': [0.8, 1],\n",
    "    'max_depth': [None],\n",
    "    'ccp_alpha': [0, 0.01],\n",
    "    'min_samples_leaf': [1,5]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(n_estimators=100, random_state=1),\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring = 'neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True,\n",
    "        verbose =3,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train.drop(columns=['stationid_int']), y_train)\n",
    "\n",
    "    print(\"Best Parameters:\", grid.best_params_)\n",
    "    print(\"Best Score:\", -grid.best_score_)\n",
    "\n",
    "    return grid.best_params_, grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params_rf, best_model_rf = random_forest_grid_search(X_train_sub, X_test_sub, y_train_sub, y_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4593395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test, regr= None, feature_columns = None, print_res = True, plot_graphs = True, target_pollutant = 'O3'):\n",
    "\n",
    "\n",
    "    if feature_columns is not None:\n",
    "        X_train = X_train[feature_columns]\n",
    "        X_test = X_test[feature_columns]\n",
    "\n",
    "    # Then create and fit model ONLY on training data\n",
    "    if regr == None:\n",
    "\n",
    "        regr = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state= 42,\n",
    "            n_jobs= -1\n",
    "        )\n",
    "\n",
    "        regr.fit(X_train.drop(columns=['stationid_int']), y_train.drop(columns=['stationid_int']))\n",
    "\n",
    "    # Predictions\n",
    "    y_train_pred = regr.predict(X_train.drop(columns=['stationid_int']))\n",
    "    y_test_pred = regr.predict(X_test.drop(columns=['stationid_int']))\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate(y_true, y_pred, print_result, dataset_type=\"Test\"):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        nrmse = rmse / y_true.mean()  \n",
    "        \n",
    "        # --- Correlations ---\n",
    "        pearson_r, _ = pearsonr(y_true, y_pred)\n",
    "        spearman_r, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "        if print_result == True:\n",
    "            print(f\"{dataset_type} MAE: {mae:.2f}\")\n",
    "            print(f\"{dataset_type} MSE: {mse:.2f}\")\n",
    "            print(f\"{dataset_type} RMSE: {rmse:.2f}\")\n",
    "            print(f\"{dataset_type} R²: {r2:.2f}\")\n",
    "            print(f\"{dataset_type} NRMSE: {nrmse:.2f}\")\n",
    "            print(f\"{dataset_type} Person Corr: {pearson_r:.2f}\")\n",
    "            print(f\"{dataset_type} Spearman Corr: {spearman_r:.2f}\")\n",
    "            \n",
    "        return r2, rmse, nrmse, pearson_r, spearman_r\n",
    "\n",
    "    r2_train, rmse_train, nrmse_train, pearson_train, spearman_train = evaluate(y_train, y_train_pred, print_res, \"Train\")\n",
    "    r2_test, rmse_test, nrmse_test, pearson_test, spearman_test = evaluate(y_test, y_test_pred, print_res, \"Test\")\n",
    "\n",
    "    def plotting():\n",
    "\n",
    "        # Feature Importance\n",
    "        importances = regr.feature_importances_\n",
    "        feature_names = X_train.drop(columns=['stationid_int']).columns\n",
    "        feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "        # plot feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "        plt.title('Feature Importances from Random Forest')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        X_test_date = X_test.copy()\n",
    "\n",
    "        X_test_date['datetime'] = pd.to_datetime(\n",
    "        dict(year=X_test_date['year'], month=X_test_date['month'], day=X_test_date['day']),\n",
    "        errors='coerce'  # invalid dates will become NaT, avoids ValueError\n",
    "        )\n",
    "        \n",
    "        random_station = np.random.choice(X_test_date['stationid_int'].unique())\n",
    "\n",
    "\n",
    "        mask = X_test_date['stationid_int'] == random_station\n",
    "        X_test_station = X_test_date.loc[mask]\n",
    "        y_test_station = y_test[mask.values]\n",
    "        y_pred_station = y_test_pred[mask.values]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_test_station[-100:], label=f'Station = {random_station} Actual {target_pollutant}',  alpha=0.7)\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_pred_station[-100:], label=f'Station = {random_station} Predicted {target_pollutant}', alpha=0.7)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(f\"{target_pollutant} Levels\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Residual Analysis\n",
    "        residuals = y_test - y_test_pred\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title('Residual Distribution')\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # plotting tree\n",
    "        tree_to_plot = regr.estimators_[0]\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plot_tree(tree_to_plot, max_depth = 3, feature_names=X_train.columns.tolist(), filled=True, rounded=True, fontsize=10)\n",
    "        plt.title(\"Decision Tree from Random Forest\")\n",
    "        plt.show()\n",
    "\n",
    "    if plot_graphs == True:\n",
    "        plotting()\n",
    "\n",
    "    return regr, r2_test, rmse_test, nrmse_test, pearson_test, spearman_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = random_forest(X_train, X_test, y_train, y_test, feature_columns= X_feature_columns, target_pollutant = target_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c7ea0",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70251d3d",
   "metadata": {},
   "source": [
    "custom recursive feature elimination from chat as ist boost regressor not compatible with rfecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "899a70eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "\n",
    "\n",
    "# --- 1️⃣ Custom wrapper around HistGradientBoostingRegressor ---\n",
    "class PermutationImportanceHGBR(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, random_state=42, n_repeats=5):\n",
    "        self.random_state = random_state\n",
    "        self.n_repeats = n_repeats\n",
    "        self.model = HistGradientBoostingRegressor(\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        # Compute permutation importance on training data\n",
    "        result = permutation_importance(\n",
    "            self.model, X, y,\n",
    "            n_repeats=self.n_repeats,\n",
    "            random_state=self.random_state,\n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        # Store feature_importances_ so RFECV can use it\n",
    "        self.feature_importances_ = np.abs(result.importances_mean)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "# --- 2️⃣ The RFECV function ---\n",
    "def recursive_feature_elimination_xg(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform Recursive Feature Elimination with Cross-Validation (RFECV)\n",
    "    using HistGradientBoostingRegressor + permutation-based importances.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use our wrapped model\n",
    "    clf = PermutationImportanceHGBR(\n",
    "        random_state=42,\n",
    "        n_repeats=1\n",
    "    )\n",
    "\n",
    "    # RFECV setup\n",
    "    rfecv = RFECV(\n",
    "        estimator=clf,\n",
    "        step=1,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        min_features_to_select=1,\n",
    "        cv=KFold(5, shuffle=True, random_state=42),\n",
    "        n_jobs=1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit\n",
    "    rfecv.fit(X_train, y_train)\n",
    "\n",
    "    # Extract selected & ranked features\n",
    "    feature_names = X_train.columns\n",
    "    feature_importances = rfecv.estimator_.feature_importances_\n",
    "    ranked_features = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": feature_importances,\n",
    "        \"Rank\": rfecv.ranking_\n",
    "    }).sort_values(\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    selected_features = ranked_features.loc[ranked_features[\"Rank\"] == 1, \"Feature\"]\n",
    "    \n",
    "    print(\"\\n✅ Selected Features:\")\n",
    "    print(selected_features.tolist())\n",
    "\n",
    "    print(\"\\n🏅 Feature Ranking:\")\n",
    "    print(ranked_features)\n",
    "\n",
    "    print(f\"\\n⭐ Optimal number of features: {rfecv.n_features_}\")\n",
    "\n",
    "    # --- Plot CV results using modern sklearn ---\n",
    "    cv_results = pd.DataFrame({\n",
    "        \"n_features\": rfecv.cv_results_[\"n_features\"],\n",
    "        \"mean_test_score\": rfecv.cv_results_[\"mean_test_score\"],\n",
    "        \"std_test_score\": rfecv.cv_results_[\"std_test_score\"]\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Mean CV Score (neg MSE)\")\n",
    "    plt.errorbar(\n",
    "        x=cv_results[\"n_features\"],\n",
    "        y=cv_results[\"mean_test_score\"],\n",
    "        yerr=cv_results[\"std_test_score\"],\n",
    "        fmt='-o'\n",
    "    )\n",
    "    plt.title(\"RFECV with Permutation Importances (HistGradientBoosting)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return selected_features, ranked_features\n",
    "\n",
    "\n",
    "# --- 3️⃣ Example usage ---\n",
    "#selected, ranked = recursive_feature_elimination_xg(X_train_sub, y_train_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "62691a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_rfecv_xgb(X_train, y_train, X_features, protected_features= ['month', 'day', 'year'], target_pollutant = 'O3', step=1, cv=5):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom RFECV that protects some features from elimination\n",
    "    and stores feature importances at each step.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : sklearn estimator\n",
    "    X_train : pd.DataFrame\n",
    "    y_train : pd.Series or array\n",
    "    protected_features : list of column names to never eliminate\n",
    "    step : int, number of features to remove per iteration\n",
    "    cv : int, cross-validation folds\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_features : list of selected features\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating validation set for perutaition importances\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = data_splitting(train_df, X_features, target_pollutant)\n",
    "\n",
    "    if protected_features is None:\n",
    "        protected_features = []\n",
    "\n",
    "    X_work = X_train_split.drop(columns=['stationid_int']).copy()\n",
    "    X_work_val = X_val_split.drop(columns=['stationid_int']).copy()\n",
    "    features_remaining = list(X_work.columns)\n",
    "    results = []\n",
    "    vali_scores = []\n",
    "    num_pollutants_list = []\n",
    "\n",
    "    step_counter = 0\n",
    "\n",
    "    while len([f for f in features_remaining if f not in protected_features]) >= step:\n",
    "\n",
    "        num_pollutants_list.append(len([f for f in features_remaining if f not in protected_features]))\n",
    "        \n",
    "        print(f'Testing with {len([f for f in features_remaining if f not in protected_features])} pollutants as features')\n",
    "\n",
    "        estimator = HistGradientBoostingRegressor(\n",
    "            random_state=42,\n",
    "            early_stopping = True,\n",
    "            n_iter_no_change = 10,\n",
    "            max_iter = 1000)\n",
    "            \n",
    "        # Compute CV score on current feature set\n",
    "        y_pred = cross_val_predict(estimator, X_work[features_remaining], y_train_split, cv=cv)\n",
    "        mse = mean_squared_error(y_train_split, y_pred)\n",
    "        vali_scores.append(mse)\n",
    "        \n",
    "        # Fit estimator\n",
    "        estimator.fit(X_work[features_remaining], y_train_split)\n",
    "\n",
    "        # Calculate permutation importance on test set\n",
    "        perm_importance = permutation_importance(estimator, X_work_val[features_remaining], y_val_split, random_state=42, n_repeats=5)\n",
    "\n",
    "        importances = pd.DataFrame({\n",
    "                'feature': features_remaining,\n",
    "                'importance': perm_importance.importances_mean\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "\n",
    "        results.append({\n",
    "            'remaining_features': features_remaining.copy(),\n",
    "            'importances': importances\n",
    "        })\n",
    "\n",
    "        # Identify candidates for elimination\n",
    "        candidates = [f for f in features_remaining if f not in protected_features]\n",
    "        n_remove = min(step, len(candidates))\n",
    "        lowest_features = importances[importances['feature'].isin(candidates)].tail(n_remove)['feature'].tolist()\n",
    "\n",
    "        # Remove lowest importance features\n",
    "        for f in lowest_features:\n",
    "            features_remaining.remove(f)\n",
    "\n",
    "        # Optional: stop when number of features = number of protected features\n",
    "        if len([f for f in features_remaining if f not in protected_features]) <= 0:\n",
    "            break\n",
    "\n",
    "    best_performance = min(vali_scores)\n",
    "    best_idx = vali_scores.index(min(vali_scores))\n",
    "    best_num_features = num_pollutants_list[best_idx]\n",
    "\n",
    "    print(f\"Optimal number of pollutants as features {best_num_features} with mse: {best_performance}\")\n",
    "\n",
    "    history_df = pd.concat([step['importances'].assign(step=i) for i, step in enumerate(results)], ignore_index=True)\n",
    "\n",
    "    # Extract features from history_df at the best step\n",
    "    optimal_step_df = history_df[history_df[\"step\"] == best_idx]\n",
    "    optimal_features = optimal_step_df[\"feature\"].tolist()\n",
    "\n",
    "    print(f\"Optimal features: {optimal_features}\")\n",
    "\n",
    "    plt.plot(num_pollutants_list, vali_scores, marker='o')\n",
    "    plt.xlabel(\"Number of pollutant features\")\n",
    "    plt.ylabel(\"Validation MSE\")\n",
    "    plt.title(\"RFECV Elimination Curve (Hist XGBoost)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_features_xgb = custom_rfecv_xgb( X_train_sub, y_train_sub, X_feature_columns,step=1,cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "84b319ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(selected, ranked)\n",
    "# top 5 features\n",
    "#hxb_important = ranked['Feature'].head(9).tolist()\n",
    "#print(hxb_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4d621955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_grid_search(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    param_grid = {\n",
    "    'max_leaf_nodes' : [31, 100, None], \n",
    "    'max_features': [0.8, 1.0],\n",
    "    'max_depth': [10, None],\n",
    "    'min_samples_leaf': [25, 100],\n",
    "    'learning_rate' : [0.1, 0.05]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator= HistGradientBoostingRegressor(max_iter=5000, random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose =3,\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train.drop(columns=['stationid_int']), y_train)\n",
    "\n",
    "    print(\"Best Parameters:\", grid.best_params_)\n",
    "    print(\"Best Score:\", -grid.best_score_)\n",
    "\n",
    "    return grid.best_params_, grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0b82d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params_xgb, best_model_xgb = xgb_grid_search(X_train_sub, X_test_sub, y_train_sub, y_test_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e86f1f",
   "metadata": {},
   "source": [
    "Plateu of performance after 9 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7480f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hxg_boost(X_train, X_test, y_train, y_test, selected_features = None, print_res = True, plot_graphs = True, model = None, target_pollutant = 'O3'):\n",
    "\n",
    "    if selected_features is not None:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "\n",
    "    if model == None:\n",
    "        # Then create and fit model ONLY on training data\n",
    "        model = HistGradientBoostingRegressor(\n",
    "            random_state=42,\n",
    "            early_stopping = True,\n",
    "            n_iter_no_change = 100,\n",
    "            max_iter = 10000\n",
    "        )\n",
    "\n",
    "        # Fit directly with NaN values\n",
    "        model.fit(X_train.drop(columns=['stationid_int']), y_train.drop(columns=['stationid_int']))\n",
    "\n",
    "    y_pred = model.predict(X_test.drop(columns=['stationid_int']))\n",
    "    \n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    nrmse = rmse/ y_test.mean()\n",
    "    # --- Correlations ---\n",
    "    pearson_r, _ = pearsonr(y_test, y_pred)\n",
    "    spearman_r, _ = spearmanr(y_test, y_pred)\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f'R²: {r2:.3f}')\n",
    "        print(f'RMSE: {rmse:.3f}')\n",
    "        print(f'NRMSE: {nrmse:.3f}')\n",
    "        print(f'MAE: {mae:.3f}')\n",
    "        print(f'Pearson Corr: {pearson_r:.3f}')\n",
    "        print(f'Spearman Corr: {spearman_r:.3f}')\n",
    "\n",
    "\n",
    "    def plotting():\n",
    "\n",
    "        # --- 4️⃣ Extract training and validation scores ---\n",
    "        train_scores = -np.array(model.train_score_)  # convert from neg MSE to positive MSE\n",
    "        val_scores = -np.array(model.validation_score_) if model.validation_score_ is not None else None\n",
    "\n",
    "        # --- 5️⃣ Plot learning curves ---\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(train_scores, label=\"Training MSE\", marker='o', alpha=0.7)\n",
    "        if val_scores is not None:\n",
    "            plt.plot(val_scores, label=\"Validation MSE\", marker='s', alpha=0.7)\n",
    "        plt.xlabel(\"Boosting Iterations\")\n",
    "        plt.ylabel(\"Mean Squared Error\")\n",
    "        plt.title(\"HistGradientBoostingRegressor Learning Curves\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        X_test_date = X_test.copy()\n",
    "\n",
    "        X_test_date['datetime'] = pd.to_datetime(\n",
    "        dict(year=X_test_date['year'], month=X_test_date['month'], day=X_test_date['day']),\n",
    "        errors='coerce'  # invalid dates will become NaT, avoids ValueError\n",
    "        )\n",
    "        \n",
    "        random_station = np.random.choice(X_test_date['stationid_int'].unique())\n",
    "\n",
    "        mask = X_test_date['stationid_int'] == random_station\n",
    "        X_test_station = X_test_date.loc[mask]\n",
    "        y_test_station = y_test[mask.values]\n",
    "        y_pred_station = y_pred[mask.values]\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_test_station[-100:], label=f'Station = {random_station} Actual {target_pollutant}',  alpha=0.7)\n",
    "        plt.plot(X_test_station['datetime'][-100:], y_pred_station[-100:], label=f'Station = {random_station} Predicted {target_pollutant}', alpha=0.7)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(f\"{target_pollutant} Levels\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate permutation importance on test set\n",
    "        perm_importance = permutation_importance(model, X_test.drop(columns=['stationid_int']), y_test, random_state=42, n_repeats=10)\n",
    "        importances = pd.Series(perm_importance.importances_mean, index=X_train.drop(columns=['stationid_int']).columns).sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\nPermutation Feature Importances:\")\n",
    "        print(importances)\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=importances.values, y=importances.index)\n",
    "        plt.title(f'Permutation Feature Importance for {target_pollutant} Prediction')\n",
    "        plt.xlabel('Importance Score (Decrease in R² when shuffled)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "\n",
    "        # Residual plot\n",
    "        residuals = y_test - y_pred\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title('Residual Distribution')\n",
    "        plt.show()\n",
    "\n",
    "    if plot_graphs == True:\n",
    "        plotting()\n",
    "\n",
    "    return model, r2, rmse, nrmse, pearson_r, spearman_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94289eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb = hxg_boost(X_train_sub, X_test_sub, y_train_sub, y_test_sub, selected_features = X_feature_columns, target_pollutant = target_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fedff",
   "metadata": {},
   "source": [
    "Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "290b0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting_reg(X_test_rf, X_test_xgb, y_test, rf_model, xgb_model, print_res = True):\n",
    "\n",
    "    # Get predictions from both models\n",
    "    y_pred_rf = rf_model.predict(X_test_rf.drop(columns=['stationid_int']))\n",
    "    y_pred_xgb = xgb_model.predict(X_test_xgb.drop(columns=['stationid_int']))\n",
    "\n",
    "    # Simple unweighted average (equal voting)\n",
    "    y_pred = (y_pred_rf + y_pred_xgb) / 2\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    nrmse = rmse/ y_test.mean()\n",
    "    # --- Correlations ---\n",
    "    pearson_r, _ = pearsonr(y_test, y_pred)\n",
    "    spearman_r, _ = spearmanr(y_test, y_pred)\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f'R²: {r2:.3f}')\n",
    "        print(f'RMSE: {rmse:.3f}')\n",
    "        print(f'NRMSE: {nrmse:.3f}')\n",
    "        print(f'MAE: {mae:.3f}')\n",
    "        print(f'Pearson Corr: {pearson_r:.3f}')\n",
    "        print(f'Spearman Corr: {spearman_r:.3f}')\n",
    "\n",
    "    return r2, rmse, nrmse, pearson_r, spearman_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pollutant(df, features, subsection = None):\n",
    "\n",
    "    # list of pollutants\n",
    "    pollutants = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['Pollutant', 'Model', 'R2', 'RMSE', 'NRMSE', 'Pearson Corr', 'Spearman Corr'])\n",
    "\n",
    "    for target_pollutant in pollutants:\n",
    "\n",
    "        print(f'Predicting: {target_pollutant}')\n",
    "\n",
    "        X_features = [col for col in features if col != target_pollutant]\n",
    "\n",
    "        if subsection == None:\n",
    "            # Change target polluatant if wanting to predict for different pollutant.\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess(df, feature_columns, target_pollutant)\n",
    "\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess_subset(df, feature_columns, target_pollutant, subset_size= subsection)\n",
    "\n",
    "        rf_model, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf = random_forest(X_train, X_test, y_train, y_test,\n",
    "                                               feature_columns= X_features, print_res = False, plot_graphs = False) \n",
    "\n",
    "        results_df.loc[len(results_df)] = [target_pollutant, 'RandomForest', r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf]\n",
    "\n",
    "        xgb_model, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb = hxg_boost(X_train,  X_test, y_train, y_test, selected_features= X_features, print_res= False, plot_graphs = False) \n",
    "\n",
    "        results_df.loc[len(results_df)] = [target_pollutant, 'HistXGboost', r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb]\n",
    "\n",
    "        r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote = voting_reg(X_test, X_test, y_test, rf_model, xgb_model, print_res = False)\n",
    "        \n",
    "        results_df.loc[len(results_df)] = [target_pollutant, 'Voting Regressor', r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote]\n",
    "\n",
    "    print(results_df)\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    axes = axes.flatten()  # flatten 2D axes array for easy indexing\n",
    "\n",
    "    # --- R² Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='R2', hue='Model', ax=axes[0],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[0].set_title('R² Comparison by Pollutant', fontsize=14)\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_xlabel('Pollutant')\n",
    "    axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].legend(title='Model')\n",
    "\n",
    "    # --- NRMSE Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='NRMSE', hue='Model', ax=axes[1],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[1].set_title('NRMSE (mean-normalized) Comparison by Pollutant', fontsize=14)\n",
    "    axes[1].set_ylabel('NRMSE (mean-normalized)')\n",
    "    axes[1].set_xlabel('Pollutant')\n",
    "    axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].legend(title='Model')\n",
    "\n",
    "    # --- Pearson r Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='Pearson Corr', hue='Model', ax=axes[2],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[2].set_title('Pearson Correlation by Pollutant', fontsize=14)\n",
    "    axes[2].set_ylabel('Pearson r')\n",
    "    axes[2].set_xlabel('Pollutant')\n",
    "    axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].legend(title='Model')\n",
    "\n",
    "    # --- Spearman r Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='Spearman Corr', hue='Model', ax=axes[3],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[3].set_title('Spearman Correlation by Pollutant', fontsize=14)\n",
    "    axes[3].set_ylabel('Spearman r')\n",
    "    axes[3].set_xlabel('Pollutant')\n",
    "    axes[3].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[3].tick_params(axis='x', rotation=45)\n",
    "    axes[3].legend(title='Model')\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c029399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all features\n",
    "#all_pollutant_results_df = all_pollutant(df, all_feature_columns)\n",
    "#df.to_csv('all_pollutant_results', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482d942",
   "metadata": {},
   "source": [
    "Full pipeline for a pollutant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bae79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(df, target_pollutant, feature_columns, subsection = None):\n",
    "\n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "    # Split data into train and test\n",
    "    print('Splitting Dataset')\n",
    "    # get target feature\n",
    "    X_features = [col for col in feature_columns if col != target_pollutant]\n",
    "\n",
    "    if subsection == None:\n",
    "        # Change target polluatant if wanting to predict for different pollutant.\n",
    "        X_train, X_test, y_train, y_test, y_scaler = preprocess(df, feature_columns, target_pollutant)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test, y_scaler = preprocess_subset(df, feature_columns, target_pollutant, subset_size= subsection)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # feature optimisation \n",
    "\n",
    "    print('Feature optimisation for Random Forest:')\n",
    "    optimal_features_rf = custom_rfecv_rf(X_train, y_train, step=1, cv=5)\n",
    "    print('Feature optimisation for HXGB')\n",
    "    optimal_features_xgb = custom_rfecv_xgb(X_train, y_train, X_features, step=1, cv=5, target_pollutant= target_pollutant)\n",
    "\n",
    "    # Need to ensure station is still tracked\n",
    "    optimal_features_rf.append('stationid_int')\n",
    "    optimal_features_xgb.append('stationid_int')\n",
    "    X_train_rf, X_test_rf = X_train[optimal_features_rf], X_test[optimal_features_rf]\n",
    "    X_train_xgb, X_test_xgb = X_train[optimal_features_xgb],  X_test[optimal_features_xgb]\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    # Grid search parmeters for selected features to predict with\n",
    "    print('Grid searching best parameters for Random Forest:')\n",
    "    best_params_rf, best_model_rf = random_forest_grid_search(X_train_rf, X_test_rf, y_train, y_test)\n",
    "    print('Grid searching best parameters for HXGB')\n",
    "    best_params_xgb, best_model_xgb = xgb_grid_search(X_train_xgb, X_test_xgb, y_train, y_test)\n",
    "    \n",
    "\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    # predicting with optimised models\n",
    "\n",
    "    print('Predicting with optimised models:')\n",
    "    results_df = pd.DataFrame(columns=['Pollutant', 'Model', 'R2', 'RMSE', 'NRMSE', 'Pearson Corr', 'Spearman Corr'])\n",
    "\n",
    "\n",
    "    rf_model, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf = random_forest(X_train_rf, X_test_rf, y_train, y_test,\n",
    "                                            feature_columns= optimal_features_rf, print_res = True, plot_graphs = True, regr = best_model_rf, target_pollutant = target_pollutant) \n",
    "\n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'RandomForest', r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf]\n",
    "\n",
    "    xgb_model, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb = hxg_boost(X_train_xgb,  X_test_xgb, y_train, y_test, selected_features= optimal_features_xgb,\n",
    "                                                                                   print_res= True, plot_graphs = True, model = best_model_xgb, target_pollutant = target_pollutant) \n",
    "\n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'HistXGboost', r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb]\n",
    "\n",
    "    r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote = voting_reg(X_test_rf, X_test_xgb, y_test, best_model_rf, best_model_xgb, print_res = False)\n",
    "    \n",
    "    results_df.loc[len(results_df)] = [target_pollutant, 'Voting Regressor', r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote]\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "    # plotting results\n",
    "\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    axes = axes.flatten()  # flatten 2D axes array for easy indexing\n",
    "\n",
    "    # --- R² Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='R2', hue='Model', ax=axes[0],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[0].set_title(f'R² for {target_pollutant} by Regression Model', fontsize=14)\n",
    "    axes[0].set_ylabel('R² Score')\n",
    "    axes[0].set_xlabel('Pollutant')\n",
    "    axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].legend(title='Model')\n",
    "\n",
    "    # --- NRMSE Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='RMSE', hue='Model', ax=axes[1],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[1].set_title(f'RMSE for {target_pollutant} by Regression Model', fontsize=14)\n",
    "    axes[1].set_ylabel('RMSE')\n",
    "    axes[1].set_xlabel('Pollutant')\n",
    "    axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].legend(title='Model')\n",
    "\n",
    "    # --- Pearson r Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='Pearson Corr', hue='Model', ax=axes[2],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[2].set_title(f'Pearson Correlation for {target_pollutant} by Regression Model', fontsize=14)\n",
    "    axes[2].set_ylabel('Pearson r')\n",
    "    axes[2].set_xlabel('Pollutant')\n",
    "    axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].legend(title='Model')\n",
    "\n",
    "    # --- Spearman r Plot ---\n",
    "    sns.barplot(data=results_df, x='Pollutant', y='Spearman Corr', hue='Model', ax=axes[3],\n",
    "                edgecolor='black', linewidth=1.2)\n",
    "    axes[3].set_title(f'Spearman Correlation for {target_pollutant} by Regression Model', fontsize=14)\n",
    "    axes[3].set_ylabel('Spearman r')\n",
    "    axes[3].set_xlabel('Pollutant')\n",
    "    axes[3].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    axes[3].tick_params(axis='x', rotation=45)\n",
    "    axes[3].legend(title='Model')\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return best_model_rf, best_model_xgb, results_df\n",
    "    \n",
    "# use all features\n",
    "#optimal_rf_model, optimal_xgb_model, optimisation_results_df = full_pipeline(df, 'NOx', all_feature_columns)\n",
    "\n",
    "#optimisation_results_df.to_csv('Results_NOx.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcba21a",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7f4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    \"\"\"\n",
    "    Linear regression model for O3 preditction.\n",
    "    Needs imputed data which does not have any Nan values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fitting model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Output model equation\n",
    "    coef = pd.Series(model.coef_, index=feature_columns)\n",
    "    intercept = model.intercept_\n",
    "\n",
    "    print(\"Linear Regression Model Formula:\")\n",
    "    eq = f\"O₃ = {intercept:.3f}\"\n",
    "    for f, c in coef.items():\n",
    "        sign = '+' if c >= 0 else '-'\n",
    "        eq += f\" {sign} {abs(c):.3f}×{f}\"\n",
    "    print(eq)\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\n📈 Model Performance after Median Imputation:\")\n",
    "    print(f\"R² = {r2:.3f}\")\n",
    "    print(f\"RMSE = {rmse:.3f}\")\n",
    "\n",
    "# Needs imputed data in form of X_train, X_test, y_train, y_test\n",
    "#linear_r2, linear_rmse = linear_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb7d02",
   "metadata": {},
   "source": [
    "svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3830b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_grid_search(X_train, X_test, y_train, y_test):\n",
    "   \n",
    "    \"\"\"\n",
    "    Perform SVR.\n",
    "    Needs iputed data with Nans removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. Parameter grid\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 50, 100],\n",
    "        'gamma': ['scale', 0.1, 0.01, 0.001],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "\n",
    "    # 4. Grid Search (Successive Halving)\n",
    "    grid = HalvingGridSearchCV(\n",
    "        estimator=SVR(kernel='rbf'),\n",
    "        param_grid=param_grid,\n",
    "        scoring='r2',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # 5. Output results\n",
    "    print(\"Best Parameters Found:\")  \n",
    "    print(grid.best_params_)\n",
    "    print(f\"Best Cross-Validation R²: {grid.best_score_:.3f}\")\n",
    "\n",
    "    return grid.best_params_, grid.best_estimator_\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Function 2: Run Standard SVR Regression Pipeline\n",
    "# ------------------------------------------------------------\n",
    "def run_svr_regression(df, feature_columns, model=None):\n",
    "    \"\"\"\n",
    "    Run SVR training + evaluation pipeline.\n",
    "    If a tuned model is provided (from svr_grid_search), it will be used directly.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Using features: {feature_columns}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Train or load model\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # Need to define svr model \n",
    "\n",
    "    if model is None:\n",
    "        print(\"Training default SVR model (C=10, gamma='scale', epsilon=0.1)...\")\n",
    "        model = SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        print(\"Using pretrained/best SVR model from grid search...\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Predict (inverse transform) \n",
    "    # Need to get inverse transform if scaling was applied in y\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "    # scaler_y needs to be defined when scaled and imported to function\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. Evaluate\n",
    "    # ------------------------------------------------------------\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"SVR Model Performance: R² = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
    "    \n",
    "    print(\"SVR Pipeline Completed: Data Cleaning + Training + Evaluation Done.\")\n",
    "\n",
    "    return model, r2, rmse\n",
    "\n",
    "\n",
    "# running model need to fix before uncommenting\n",
    "\n",
    "# best_params, best_model = svr_grid_search_with_data(X_train, y_train)\n",
    "# model, r2, rmse = run_svr_regression(X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53d755",
   "metadata": {},
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d41edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_grid_search_with_data(X_train, y_train):\n",
    "  \n",
    "\n",
    "    # Need to use imputed data\n",
    "\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski', 'manhattan', 'euclidean']\n",
    "    }\n",
    "\n",
    "    # 4. Grid Search (successive halving)\n",
    "    grid = HalvingGridSearchCV(\n",
    "        estimator=KNeighborsRegressor(),\n",
    "        param_grid=param_grid,\n",
    "        scoring='r2',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Best Parameters Found:\")\n",
    "    print(grid.best_params_)\n",
    "    print(f\"Best Cross-Validation R²: {grid.best_score_:.3f}\")\n",
    "\n",
    "    return grid.best_params_, grid.best_estimator_\n",
    "\n",
    "def run_knn_regression(X_train, X_test, y_train, y_test):\n",
    "   \n",
    "    # Need to use and imputed and potentially scaled data\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. Train KNN model\n",
    "    # ------------------------------------------------------------\n",
    "    knn_model = KNeighborsRegressor(\n",
    "        n_neighbors=5,      # Number of neighbors\n",
    "        weights='distance', # Weighting method\n",
    "        metric='minkowski'  # Distance metric\n",
    "    )\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Prediction\n",
    "    # ------------------------------------------------------------\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Evaluation\n",
    "    # ------------------------------------------------------------\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nKNN Model Performance: R² = {r2:.3f}, RMSE = {rmse:.3f}\")\n",
    "\n",
    "    return knn_model, r2, rmse\n",
    "\n",
    "# Usage needs imputed data and potenially scaled data\n",
    "\n",
    "#best_params, best_model = knn_grid_search_with_data(X_train, y_train)\n",
    "#model, r2, rmse = run_knn_regression(X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638eb604",
   "metadata": {},
   "source": [
    "Neural Net/ lstm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0f3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6089744",
   "metadata": {},
   "source": [
    "### comparing results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64578197",
   "metadata": {},
   "source": [
    "Comparing prediction performance with removal of correlated features to see if model can still perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "387d393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a35fdd",
   "metadata": {},
   "source": [
    "Removing one of the two features that have correlation greater than 0.8. \n",
    "\n",
    "The removed feature will be based on the one with fewer NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17341a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_features_to_remove(df, threshold_corr = 0.8, target_pollutant = 'O3', print_res = False):\n",
    "\n",
    "    # list of pollutants\n",
    "    pollutants = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    # getting corralation\n",
    "    corr = df[pollutants].corr()\n",
    "\n",
    "    # 2. Keep only upper triangle (exclude diagonal & duplicates)\n",
    "\n",
    "    corr = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    corr = corr[corr >= threshold_corr]\n",
    "\n",
    "    # 3. Flatten into long form (pairs + correlation value)\n",
    "    corr_pairs = (\n",
    "        corr.stack()  # removes NaN automatically\n",
    "        .reset_index()\n",
    "    )\n",
    "    corr_pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "    \n",
    "    feature_to_remove_by_Nan = []\n",
    "    feature_to_remove_by_target_corr = []\n",
    "\n",
    "    # iterate through pairs of highly correlated features\n",
    "    for pair in corr_pairs.index:\n",
    "\n",
    "        f1, f2 = corr_pairs.loc[pair, ['Feature_1', 'Feature_2']]\n",
    "\n",
    "        if f1 == target_pollutant or f2 == target_pollutant:\n",
    "            continue\n",
    "\n",
    "        # Count NaN values for each feature in the original DataFrame\n",
    "        nan_f1 = df[f1].isna().sum()\n",
    "        nan_f2 = df[f2].isna().sum()\n",
    "\n",
    "        # Compare which has more missing values\n",
    "        if nan_f1 > nan_f2:\n",
    "            if print_res == True:\n",
    "                print(f\"{f1} has more NaN values ({nan_f1}) than {f2} ({nan_f2}).\")\n",
    "            feature_to_remove_by_Nan.append(f1)\n",
    "        else:\n",
    "            if print_res == True:\n",
    "                print(f\"{f2} has more NaN values ({nan_f2}) than {f1} ({nan_f1}).\")\n",
    "            feature_to_remove_by_Nan.append(f2)\n",
    "\n",
    "        # Compute correlation with target \n",
    "        corr_f1 = df[[f1, target_pollutant]].corr().iloc[0, 1]\n",
    "        corr_f2 = df[[f2, target_pollutant]].corr().iloc[0, 1]\n",
    "\n",
    "        # Compare absolute correlation with the target\n",
    "        if corr_f1 > corr_f2:\n",
    "            if print_res == True:\n",
    "                print(f\"{f2} has lower correlation ({corr_f2:.3f}) with {target_pollutant} than {f1} ({corr_f1:.3f}). Removing {f2}.\")\n",
    "            feature_to_remove_by_target_corr.append(f2)\n",
    "        else:\n",
    "            if print_res == True:\n",
    "                print(f\"{f1} has lower correlation ({corr_f1:.3f}) with {target_pollutant} than {f2} ({corr_f2:.3f}). Removing {f1}.\")\n",
    "            feature_to_remove_by_target_corr.append(f1)\n",
    "\n",
    "\n",
    "    return corr_pairs, [feature_to_remove_by_Nan, feature_to_remove_by_target_corr]\n",
    "    \n",
    "def compare_w_removed_features(df, features, subsection = None):\n",
    "\n",
    "    # list of pollutants\n",
    "    pollutants = ['PM2.5','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['Pollutant', 'Model', 'Removal_reason', 'R2', 'RMSE', 'NRMSE', 'Pearson Corr', 'Spearman Corr'])\n",
    "\n",
    "    for target_pollutant in pollutants:\n",
    "\n",
    "        print(f'Predicting: {target_pollutant}')\n",
    "\n",
    "        if subsection == None:\n",
    "            # Change target polluatant if wanting to predict for different pollutant.\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess(df, feature_columns, target_pollutant)\n",
    "\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test, y_scaler = preprocess_subset(df, feature_columns, target_pollutant, subset_size= subsection)\n",
    "\n",
    "        # get info on highly correlated pairs \n",
    "        corr_pairs, features_to_remove = corr_features_to_remove(df, target_pollutant = target_pollutant, print_res = False)\n",
    "        \n",
    "        # Add a baseline \"no removal\" condition\n",
    "        features_to_remove.insert(0, [])  \n",
    "        removal_methods = ['None', 'Number of NaN', 'Corr w Target']\n",
    "\n",
    "        for i, method in enumerate(removal_methods):\n",
    "            \n",
    "            remove_list = features_to_remove[i]\n",
    "            \n",
    "            X_features = [col for col in features \n",
    "              if col != target_pollutant and col not in remove_list]\n",
    "            \n",
    "\n",
    "\n",
    "            X_train_iter, X_test_iter = X_train[X_features], X_test[X_features]\n",
    "\n",
    "            rf_model, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf = random_forest(X_train_iter, X_test_iter, y_train, y_test,\n",
    "                                                feature_columns= X_features, print_res = False, plot_graphs = False) \n",
    "\n",
    "            results_df.loc[len(results_df)] = [target_pollutant, 'RandomForest', method, r2_rf, rmse_rf, nrmse_rf, pearson_rf, spearman_rf]\n",
    "\n",
    "            xgb_model, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb = hxg_boost(X_train_iter,  X_test_iter, y_train, y_test, selected_features= X_features, print_res= False, plot_graphs = False) \n",
    "\n",
    "            results_df.loc[len(results_df)] = [target_pollutant, 'HistXGboost', method, r2_xgb, rmse_xgb, nrmse_xgb, pearson_xgb, spearman_xgb]\n",
    "\n",
    "\n",
    "            # Dont need votign regressor\n",
    "            #r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote = voting_reg(X_test_iter, X_test_iter, y_test, rf_model, xgb_model, print_res = False)\n",
    "            \n",
    "            #results_df.loc[len(results_df)] = [target_pollutant, 'Voting Regressor', method, r2_vote, rmse_vote, nrmse_vote, pearson_vote, spearman_vote]\n",
    "\n",
    "    return results_df\n",
    "\n",
    "#corr_pairs, feat_to_remove = corr_features_to_remove(df, print_res = True, target_pollutant= 'PM10')\n",
    "\n",
    "# use all features\n",
    "#results_remove_corr = compare_w_removed_features(df, all_feature_columns, subsection = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c5510a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_remove_corr(results_df):\n",
    "    \n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    g = sns.catplot(\n",
    "        data=results_df,\n",
    "        x=\"Pollutant\",\n",
    "        y=\"R2\",\n",
    "        hue=\"Removal_reason\",\n",
    "        col=\"Model\",\n",
    "        kind=\"bar\",\n",
    "        height=5,\n",
    "        aspect=1.1,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    g.fig.set_dpi(150)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g.set_axis_labels(\"Pollutant\", \"R²\")\n",
    "    g.add_legend(title=\"Removal reason\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plotting_remove_corr(results_remove_corr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
